{"cells":[{"cell_type":"markdown","metadata":{"id":"IZG7hNuyuPCJ"},"source":["# Thực hiện xây dựng mạng Neural Network\n","\n","Trong tuần này, bạn sẽ thực hiện xây dựng Neural Network đơn giản với:\n","- 2 lớp ẩn với hàm phi tuyến Relu\n","- một lớp phân loại softmax\n","\n","Bạn có thể mở rộng notebook này để có mạng nhiều lớp hơn."]},{"cell_type":"markdown","metadata":{"id":"tJk5UrQ6nXJK"},"source":["Chú ý: Bài tập có một số thiếu sót và cách khắc phục ở dưới đây.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Boozh29bidzo"},"source":["**Fix bug**:\n","\n","Xem chi tiết trong TODO 8\n","\n","- TODO 8:\n","\n","```Python\n","db3 = 1/m * np.sum(E3, keepdims=True, axis=1)\n","```\n","\n","- TODO 8:\n","\n","```Python\n","m = X_train.shape[0]\n","```"]},{"cell_type":"markdown","metadata":{"id":"lZTfsvxWssun"},"source":["## 1 - Khai báo một số hàm bổ sung"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MEseTbtQplF8"},"outputs":[],"source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"XpT1677QuPCV"},"source":["## 2 - Quá trình thực hiện\n","\n","Quá trình toàn bộ mô hình sẽ theo các bước sau\n","\n","- Khởi tạo các tham số W, b.\n","- Triển khai `Forward Propagation Module` (lan truyền thuận). Như đường màu tím trong hình bên dưới.\n","- Tính giá trị mất mát cost.\n","- Thực hiện `Backward Propagation Module` (lan truyền ngược). Như đường màu xanh trong hình bên dưới.\n","- Cập nhật các tham số.\n","\n","<img src=\"https://storage.googleapis.com/protonx-cloud-storage/images/Feedforward.PNG\" style=\"width:800px;height:500px;\">\n","<caption><center> Ảnh 1</center></caption><br>\n","\n","**Lưu ý**: với mỗi forward function, sẽ có backward function tương ứng. Đó là lý do mà ở mỗi forward module chúng ta sẽ lưu lại các giá trị này trong `cache` để thuận tiện cho việc tính `gradients`.\n"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"KKhUIXrkuPCW"},"source":["## 3 - Khởi tạo\n"]},{"cell_type":"markdown","metadata":{"id":"7rimzl3wumQo"},"source":["**Hướng dẫn**:\n","- Khởi tạo giá trị random cho ma trận. Sử dụng `np.random.randn(shape)*0.01`.\n","- Khởi tạo `0` cho bias. Sử dụng `np.zeros(shape)`."]},{"cell_type":"markdown","metadata":{"id":"Y_x3JL9zkGYu"},"source":["Các bộ tham số giữa các lớp sẽ có chiều như sau:\n","\n","- Lớp 1:\n","  - $\\textbf{W}^{(1)} \\in \\mathbf{R}^{d^{(1)} \\times  n } $\n","  - $b^{(1)} \\in \\mathbf{R}^{ d^{(1)} \\times 1 }$\n","- Lớp 2:\n","  - $\\textbf{W}^{(2)} \\in \\mathbf{R}^{d^{(2)} \\times d^{(1)}} $\n","  - $b^{(2)} \\in \\mathbf{R}^{ d^{(2)} \\times 1 }$\n","- Lớp 3:\n","  - $\\textbf{W}^{(3)} \\in \\mathbf{R}^{c \\times d^{(2)}} $\n","  - $b^{(3)} \\in \\mathbf{R}^{ c \\times 1 }$\n","\n","Biến `num_classes` đại diện cho số nhãn $c$ trong trường hợp này."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSoEHkUJuPCX"},"outputs":[],"source":["def initialize_parameters(n, d_1, d_2, num_classes):\n","    \"\"\"\n","    Hàm khởi tạo tham số ban đầu\n","    Đầu vào:\n","      n: int\n","        Chiều của x đầu vào\n","      d_1: int\n","        Chiều của lớp ẩn 1\n","      d_2:\n","        Chiều của lớp ẩn 2\n","      num_classes:\n","        Số lượng nhãn\n","    Đầu ra:\n","      parameters: python dictionary\n","      Bao gồm:\n","        W1: ma trận W1 lớp 1 với chiều (d_1, n)\n","        b1: vector bias b1 lớp 1 với chiều (d_1, 1)\n","        W2: ma trận W2 lớp 2 với chiều (d_2, d_1)\n","        b2: vector bias b2 lớp 2 với chiều (d_2, 1)\n","        W3:  ma trận W3 lớp 3 với chiều (num_classes, d_2)\n","        b3: vector bias b3 lớp 3 với chiều (num_classes, 1)\n","    \"\"\"\n","\n","    # Khởi tạo tham số\n","\n","    np.random.seed(42)\n","    W1 = np.random.randn(d_1, n) * 0.01\n","    b1 = np.zeros((d_1, 1))\n","    W2 = np.random.randn(d_2, d_1) * 0.01\n","    b2 = np.zeros((d_2, 1))\n","    W3 = np.random.randn(num_classes, d_2) * 0.01\n","    b3 = np.zeros((num_classes, 1))\n","\n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2,\n","                  \"W3\": W3,\n","                  \"b3\": b3,\n","                  }\n","\n","    return parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1698999876527,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"1H9ylZ8vuPCc","outputId":"797ab840-9dcb-423f-8898-bce70fdd2afa","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["W1.shape = (256, 784)\n","b1.shape = (256, 1)\n","W2.shape = (128, 256)\n","b2.shape = (128, 1)\n","W3.shape = (10, 128)\n","b3.shape = (10, 1)\n"]}],"source":["parameters = initialize_parameters(784, 256, 128, 10)\n","print(\"W1.shape = {}\".format(parameters[\"W1\"].shape))\n","print(\"b1.shape = {}\".format(parameters[\"b1\"].shape))\n","print(\"W2.shape = {}\".format(parameters[\"W2\"].shape))\n","print(\"b2.shape = {}\".format(parameters[\"b2\"].shape))\n","print(\"W3.shape = {}\".format(parameters[\"W3\"].shape))\n","print(\"b3.shape = {}\".format(parameters[\"b3\"].shape))"]},{"cell_type":"markdown","metadata":{"id":"BA0wIS9ouPCs"},"source":["## 4 - Lan truyền thuận (Forward propagation module)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xJ4OlH6Ru3uB"},"source":["### 4.1 - Lan truyền thuận tuyến tính (Linear Forward)\n","\n","Sau khi khởi tạo các tham số cần thiết, bây giờ bạn sẽ thực hiện forward propagation module. Bạn sẽ thực hiện theo thứ tự sau:\n","\n","- Tuyến tính (Linear)\n","- Tuyến tính $\\rightarrow $ Phi tuyến (Linear $\\rightarrow $ Activation)\n","\n","\n","#### 4.1.1. Trên 1 điểm dữ liệu\n","\n","\n","Công thức lan truyền thuận tuyến tính như sau:\n","\n","$$\\textbf{z}^{(l)} = \\textbf{W}^{(l)}\\textbf{a}^{(l-1)} +\\textbf{b}^{(l)}\\tag{4}$$\n","\n","trong đó $\\textbf{a}^{(0)} = \\textbf{x}$.\n","\n","\n","#### 4.1.2. Trên toàn tập dữ liệu\n","\n","\n","Công thức Linear Forward như sau:\n","\n","\n","\n","$$\\textbf{Z}^{(l)} = \\textbf{W}^{(l)}\\textbf{A}^{(l-1)} +\\textbf{b}^{(l)}\\tag{5}$$\n","\n","\n","trong đó $\\textbf{A}^{(0)} = \\textbf{X}$\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iB3SvCIhuPCt"},"outputs":[],"source":["def linear_forward(A_pre, W, b):\n","    \"\"\"\n","    Tiến hành lan truyền thuận tuyến tính (linear forward)\n","    Đầu vào:\n","      A_pre:\n","        Dạng: numpy array\n","        Miêu tả: Đầu ra của lớp phía trước trên toàn bộ tập dữ liệu\n","        Chiều: (Chiều của lớp phía trước, Số lượng điểm dữ liệu)\n","      W:\n","        Dạng: numpy array\n","        Miêu tả: Ma trận tham số của lớp hiện tại\n","        Chiều: (Chiều của lớp hiện tại, Chiều của lớp phía trước)\n","      b:\n","        Dạng: numpy array\n","        Miêu tả: Vector bias của lớp hiện tại\n","        Chiều: (Chiều của lớp hiện tại, 1)\n","    Đầu ra:\n","      Z:\n","        Dạng: numpy array\n","        Miêu tả: Đầu ra của phép nhân tuyến tính và cũng là đầu vào của hàm activation\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","      cache:\n","        Dạng: python tuple\n","        Miêu tả: Bao gồm các biến A_pre, W và b\n","    \"\"\"\n","\n","    Z = np.dot(W, A_pre) + b\n","\n","    cache = (A_pre, W, b)\n","\n","    return Z, cache"]},{"cell_type":"markdown","metadata":{"id":"l4CtxMAHUGw0"},"source":["Test code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1698999876527,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"mPxh6ht6uPCx","outputId":"87feff1f-8d18-4cf7-da12-dd5ccec85a09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Z.shape: (128, 60000)\n"]}],"source":["try:\n","  A1 = np.ones((256, 60000))\n","  W2 = np.random.normal(size=((128, 256)))\n","  b2 = np.ones(((128, 1)))\n","\n","  Z, linear_cache = linear_forward(A1, W2, b2)\n","  print(\"Z.shape: {}\".format(Z.shape))\n","except Exception as e:\n","  print(\"Lỗi thực thi: {}\", e)"]},{"cell_type":"markdown","metadata":{"id":"LXNBmND6fV-z"},"source":["### 4.2. Lan truyền thuận phi tuyến (Activation Forward)"]},{"cell_type":"markdown","metadata":{"id":"whvJKWdLfoXA"},"source":["#### 4.2.1 Công thức RELU\n","\n","$$f(z) = \\left\\{\\begin{matrix}\n","0 \\ \\forall z \\leq 0 \\\\\n","z \\ \\forall z > 0 \\\\\n","\\end{matrix}\\right. = \\max\\left \\{0, z  \\right \\} = z\\textbf{1}_{z>1}$$"]},{"cell_type":"markdown","metadata":{"id":"TyPiNkq7gNhv"},"source":["**Chú ý:** Các phép tính trong bài toán này đều sử dụng ma trận cho thuật toán Gradient Descent thay vì vector trong bài tập Softmax"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G7Csx0CHoeZn"},"outputs":[],"source":["def relu(Z):\n","    \"\"\"\n","    Thực hiện hàm RELU trên ma trận Z\n","    Đầu vào:\n","      Z:\n","        Dạng: numpy array\n","        Miêu tả: Ma trận Z. Đầu ra của phép nhân tuyến tính\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","        Ví dụ: (256, 60000)\n","    Đầu ra:\n","      A:\n","        Dạng: numpy array\n","        Miêu tả: Giá trị A sau khi đưa Z qua Relu\n","        Chiều: (Chiều của lớp hiện tại, 60000)\n","        Ví dụ: (256, 60000)\n","      cache:\n","        Dạng: numpy array\n","        Miêu tả:\n","          Ma trận Z. Đầu ra của phép nhân tuyến tính (hay đầu vào của phép phi tuyến)\n","          Việc lưu trữ này để sử dụng cho thuật toán lan truyền ngược\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","        Ví dụ: (256, 60000)\n","    \"\"\"\n","\n","    # Lập trình tại đây\n","\n","    A = np.maximum(0,Z)\n","\n","    cache = Z\n","\n","    return A, cache"]},{"cell_type":"markdown","metadata":{"id":"_XONCPgw-Qsf"},"source":["#### 4.2.2. Công thức Softmax\n"]},{"cell_type":"markdown","metadata":{"id":"qK1_psFQgFr8"},"source":["\n","Trong trường hợp này ta sẽ lấy softmax trên từng cột của $\\textbf{Z}$ với $\\textbf{Z} \\in \\mathbf{R}^{c \\times m} $\n","\n","Với:\n","\n","- $c$: Số nhãn\n","- $m$: Số lượng điễm dữ liệu\n","\n","\n","$$\\hat{\\textbf{Y}} = \\text{softmax}(\\textbf{Z}) =  \\begin{bmatrix}\n","\\text{softmax}(\\begin{bmatrix}\n","z_1^1 \\\\\n","\\ \\vdots \\\\\n","z_c^1 \\\\\n","\\end{bmatrix}) & \\text{softmax}(\\begin{bmatrix}\n","z_1^2 \\\\\n","\\ \\vdots \\\\\n","z_c^2 \\\\\n","\\end{bmatrix}) & \\dots & \\text{softmax}(\\begin{bmatrix}\n","z_1^m \\\\\n","\\ \\vdots \\\\\n","z_c^m \\\\\n","\\end{bmatrix})\n","\\end{bmatrix} $$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_wmZVN138GKW"},"outputs":[],"source":["def softmax(Z):\n","  \"\"\"\n","  Thực hiện hàm softmax trên ma trận Z\n","  Đầu vào:\n","    Z:\n","      Dạng: numpy array\n","      Miêu tả: Ma trận Z. Đầu ra của phép nhân tuyến tính\n","      Chiều: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","      Ví dụ: (10, 60000)\n","  Đầu ra:\n","    Y_hat:\n","      Dạng: numpy array\n","      Miêu tả: Chuẩn hóa ma trận Z theo phân phối softmax có tổng các giá trị bằng 1\n","      Chiều: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","      Ví dụ: (10, 60000)\n","    cache:\n","      Dạng: numpy array\n","      Miêu tả:\n","        Ma trận Z. Đầu ra của phép nhân tuyến tính\n","        Việc lưu trữ này để sử dụng cho thuật toán lan truyền ngược\n","      Chiều: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","      Ví dụ: (10, 60000)\n","  \"\"\"\n","\n","  e_Z = np.exp(Z)\n","\n","  Y_hat = e_Z/ e_Z.sum(axis=0)\n","\n","  print(e_Z.shape, e_Z.sum(axis=0).shape)\n","\n","  cache = Z\n","\n","  return Y_hat, cache"]},{"cell_type":"markdown","metadata":{"id":"j3rPCVP3jGw1"},"source":["Test code\n"]},{"cell_type":"markdown","metadata":{"id":"mNcZEbYY-c3k"},"source":["Kiểm tra lại các cột sau khi sử dụng Softmax có cho kết quả tổng bằng 1 hay không."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1698999876528,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"vQm9e4cv8ItL","outputId":"3c86865f-eff8-47fa-f3be-64ed306b5077"},"outputs":[{"output_type":"stream","name":"stdout","text":["(10, 60000) (60000,)\n"]},{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{},"execution_count":8}],"source":["Z = np.ones(((10, 60000)))\n","Y_hat, _ = softmax(Z)\n","\n","np.sum(Y_hat[:,3])"]},{"cell_type":"markdown","metadata":{"id":"EJMHwVRIuPC3"},"source":["### 4.3 - Lan truyền thuận trên một lớp (Linear-Activation Forward)\n","\n","Trong notebook này, bạn sẽ sử dụng activation function `RELU`:\n","\n","<!-- - **Sigmoid**: $ \\text{Sigmoid}(\\textbf{Z}) = \\sigma(\\textbf{Z}) = \\sigma(\\textbf{W} \\textbf{A} + \\textbf{b}) = \\frac{1}{ 1 + e^{-(\\textbf{W} \\textbf{A} + \\textbf{b})}}$. Function này sẽ trả về 2 giá trị \"`A`\" (giá trị activation) và \"`cache`\" (chứa giá trị của `Z`) - Các giá trị này sẽ được sử dụng cho Backward Function. Để sử dụng, bạn chỉ cần gọi theo mẫu sau:\n","```python\n","A, activation_cache = sigmoid(Z)\n","``` -->\n","\n","- **ReLU**: công thức toán của ReLU là $\\textbf{A} = \\text{Relu}(\\textbf{Z}) = \\text{max}(0, \\textbf{Z})$.  Function này sẽ trả về 2 giá trị \"`a`\" (giá trị activation) và \"`cache`\" (chứa giá trị của `Z`) - Các giá trị này sẽ được sử dụng cho Backward Function. Để sử dụng, bạn chỉ cần gọi theo mẫu sau:\n","``` python\n","A, activation_cache = relu(Z)\n","```"]},{"cell_type":"markdown","metadata":{"id":"sexU0YLZSmWm"},"source":["- Lớp 1:\n","\n","$$\\textbf{A}^{(1)} = \\text{Relu}(\\textbf{Z}^{(1)}) $$\n","\n","- Lớp 2:\n","\n","$$\\textbf{A}^{(2)} = \\text{Relu}(\\textbf{Z}^{(2)}) $$\n","\n","- Lớp 3 (Lớp cuối):\n","\n","$$\\textbf{A}^{(3)} = \\hat{\\textbf{Y}} = \\text{Softmax}(\\textbf{Z}^{(3)}) \\tag{6}$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4WefOTnWuPC4"},"outputs":[],"source":["def linear_activation_forward(A_prev, W, b, activation):\n","    \"\"\"\n","    Tiến hành non-linear forward\n","    A_prev, W và B đưa qua hàm linear_forward thu được Z\n","    Từ đó Z sẽ được đưa qua hàm phi tuyến RELU\n","    Đầu vào:\n","      A_prev:\n","        Dạng: numpy array\n","          Miêu tả: Đầu ra của lớp phía trước trên toàn bộ tập dữ liệu\n","          Chiều: (Chiều của lớp phía trước, Số lượng điểm dữ liệu)\n","      W:\n","        Dạng: numpy array\n","        Miêu tả: Ma trận tham số của lớp hiện tại\n","        Chiều: (Chiều của lớp hiện tại, Chiều của lớp phía trước)\n","      b:\n","        Dạng: numpy array\n","        Miêu tả: Vector bias của lớp hiện tại\n","        Chiều: (Chiều của lớp hiện tại, 1)\n","      activation:\n","        Dạng: python string\n","        Miêu tả: Tên hàm activation. Ví dụ \"softmax\" or \"relu\"\n","\n","    Returns:\n","      A:\n","        Dạng: numpy array\n","        Miêu tả: Đầu ra của phép phi tuyến hay đầu ra của hàm activation function\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","      cache:\n","        Dạng: python tuple: (linear_cache, activation_cache)\n","        Bao gồm:\n","          linear_cache: tuple (A_pre, W, b) đầu vào của phép tuyến tính được lưu lại\n","          activation_cache: Z đầu vào của phép phi tuyến được lưu lại\n","    \"\"\"\n","    # Ta sẽ phân loại việc biến đổi tuyến tính tùy theo đầu vào activation\n","    if activation == \"relu\":\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","    elif activation == \"softmax\":\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = softmax(Z)\n","\n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache"]},{"cell_type":"markdown","metadata":{"id":"IabUnrabXmop"},"source":["Test code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1698999876528,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"0jWyGRr_uPC7","outputId":"216f7515-2209-487e-8be0-39b2d4214406"},"outputs":[{"output_type":"stream","name":"stdout","text":["(10, 60000) (60000,)\n","A.shape: (10, 60000)\n","activation_cache.shape hay Z.shape: (10, 60000)\n","linear_cache bao gồm A2 với chiều A2.shape: (256, 60000), W3 với chiều W3.shape: (10, 256), b3 với chiều b3.shape: (10, 1)\n"]}],"source":["try:\n","  A2 = np.ones((256, 60000))\n","  W3 = np.random.normal(size=((10, 256)))\n","  b3 = np.ones(((10, 1)))\n","\n","  A, cache = linear_activation_forward(A2, W3, b3, activation='softmax')\n","  print(\"A.shape: {}\".format(A.shape))\n","\n","  # cache là một tuple\n","\n","  linear_cache, activation_cache = cache\n","  A2, W3, b3 = linear_cache\n","\n","  print(\"activation_cache.shape hay Z.shape: {}\".format(activation_cache.shape))\n","  print(\"linear_cache bao gồm A2 với chiều A2.shape: {}, W3 với chiều W3.shape: {}, b3 với chiều b3.shape: {}\".format(A2.shape, W3.shape, b3.shape))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"PwsazF-kuPDI"},"source":["## 5 - Hàm mất mát (Cost function)\n","\n","Công thức toán của cross-entropy cost $J$: $$J(\\textbf{W}) = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\sum\\limits_{j = 1}^{c} \\textbf{y}^{(i)}\\log (\\hat{\\textbf{y}}^{(i)})\\tag{7}$$\n","\n","Với\n","- $c$: số nhãn\n","- $m$: số điểm dữ liệu\n","\n","Chú ý trong bài này đầu vào của hàm là theo toàn bộ tập dữ liệu.\n"]},{"cell_type":"markdown","metadata":{"id":"KT_l4MKbNC9v"},"source":["**TODO 1**: Lập trình công thức hàm Cost Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qYYnRcouPDI"},"outputs":[],"source":["def compute_cost(Y_hat, Y):\n","    \"\"\"\n","    Tiến hành categorical cross-entropy trên toàn bộ tập dữ liệu\n","    Đầu vào:\n","    Y_hat:\n","      Dạng: numpy array\n","      Miêu tả: Đầu ra của lớp softmax trên toàn bộ tập dữ liệu\n","      Chiều: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","    Y:\n","      Dạng: numpy array\n","      Miêu tả: Nhãn của dữ liệu (Dạng One hot)\n","      Chiều: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","\n","    Returns:\n","      Dạng: numpy float\n","      Miêu tả: Giá trị mất mát trên toàn bộ tập dữ liệu\n","    \"\"\"\n","\n","    # Lập trình tại đây\n","\n","    m = Y.shape[1]\n","\n","    # Nhân Hadamard và lấy tổng theo cột.\n","    # Lấy tổng theo cột vì chiều đầu vào có số dòng là số lượng nhãn cho nên ta\n","    # cộng tất cả giá trị trong cùng 1 cột lại với nhau để tìm giá trị mất mát\n","    # trên 1 điểm dữ liệu đó\n","\n","    cost = np.sum((Y * np.log(Y_hat)), axis=0)\n","\n","\n","    # Lấy tổng theo theo cột kèm theo chia trung bình\n","    cost = - 1/m * np.sum(cost)\n","\n","    return cost"]},{"cell_type":"markdown","metadata":{"id":"7bhcwVHWYlgW"},"source":["Test code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1698999876528,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"nbqLdQsjuPDK","outputId":"085eab19-9d38-4d3e-caa6-76cbe57a7bd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["cost = 2.3025850929940455\n"]}],"source":["Y =  np.array(\n","      [[0., 0.],\n","       [0., 0.],\n","       [0., 1.],\n","       [0., 0.],\n","       [0., 0.],\n","       [1., 0.],\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.]])\n","\n","Y_hat =  np.array(\n","      [[0.1, 0.05],\n","       [0.1, 0.05],\n","       [0.1, 0.05],\n","       [0.2, 0.2],\n","       [0.05, 0.05],\n","       [0.2, 0.1],\n","       [0.1, 0.1],\n","       [0.05, 0.2],\n","       [0.05, 0.1],\n","       [0.05, 0.1]])\n","\n","print(\"cost = {}\".format(compute_cost(Y_hat, Y)))"]},{"cell_type":"markdown","metadata":{"id":"NmUY02AzNXqB"},"source":["**Kết quả mong đợi**:\n","\n","```\n","cost = 2.3025850929940455\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"iqtGNVroYm0h"},"source":["Giá trị này tương đương với $-(\\text{log}(0.2) + \\text{log}(0.05))$"]},{"cell_type":"markdown","metadata":{"id":"7f-Ms322uPDN"},"source":["## 6 - Lan truyền ngược (Backward propagation module)\n","\n","Cũng giống như lan truyền thuận (forward propagation), bây giờ bạn sẽ thực hiện các function cho backpropagation. Chú ý lan truyền thuận được sử dụng để tính toán gradient của hàm mất mát với các tham số (parameters).\n"]},{"cell_type":"markdown","metadata":{"id":"VAh6SysJZJ1Z"},"source":["Mỗi một hàm lan truyền thuận phía trên sẽ đi kèm một hàm lan truyền ngược"]},{"cell_type":"markdown","metadata":{"id":"axA8WuxqZizJ"},"source":["### 6.1 - Lan truyền ngược tuyến tính (Linear backward)"]},{"cell_type":"markdown","metadata":{"id":"lj8N7e2suPDN"},"source":["\n","\n","<img src=\"https://storage.googleapis.com/protonx-cloud-storage/images/Backprop3.PNG\" style=\"width:250px;height:300px;\">\n","<caption><center>Hình 4</center></caption>\n","\n","Tại lớp thứ $l$, ta có: $\\textbf{Z}^{(l)} = \\textbf{W}^{(l)} \\textbf{A}^{(l-1)} + \\textbf{b}^{(l)}$ (Activation tương ứng).\n","\n","Nhiệm vụ tiên quyết là tính được các giá trị $\\textbf{E}^{(l)}$ vì ta có thể dùng $\\textbf{E}^{(l)}$ để tính ra được $\\textbf{E}^{(l-1)}$\n","\n","Giá sử bạn đã tính đạo hàm của $\\textbf{E}^{(l)} = d\\textbf{Z}^{(l)} = \\frac{\\partial J }{\\partial \\textbf{Z}^{(l)}}$.\n","\n","\n","\n","Bạn muốn tính $(d\\textbf{W}^{(l)}, d\\textbf{b}^{(l)}, d\\textbf{A}^{(l-1)})$.\n","\n","3 giá trị $(d\\textbf{W}^{(l)}, d\\textbf{b}^{(l)}, d\\textbf{A}^{(l-1)}$ được tính toán từ $d\\textbf{Z}^{(l)}$ thông qua các công thức sau:\n","\n","\n","- Đạo hàm của hàm mất mát trên $\\textbf{W}^{(l)}$\n","\n","$$\n","d\\textbf{W}^{(l)} = \\frac{\\partial {J} }{\\partial \\textbf{W}^{(l)}} = \\frac{1}{m} d\\textbf{Z}^{(l)} \\textbf{A}^{(l-1) T} = \\frac{1}{m} \\textbf{E}^{(l)} \\textbf{A}^{(l-1) T} \\tag{8}\n","$$\n","\n","- Đạo hàm của hàm mất mát trên $\\textbf{b}^{(l)}$.Một cách cụ thể thì đây chính là trung bình tổng các cột của ma trận ${E}^{(l)}$\n","\n","\n","$$ d\\textbf{b}^{(l)} = \\frac{\\partial J }{\\partial \\textbf{b}^{(l)}} = \\frac{1}{m} \\sum_{i = 1}^{m} d\\textbf{Z}^{(l)(i)} = \\frac{1}{m} \\sum_{i = 1}^{m} \\textbf{E}^{(l)(i)} \\tag{9}$$\n","\n","      \n","- Đạo hàm của hàm mất mát trên $\\textbf{A}^{(l-1)}$\n","\n","$$ d\\textbf{A}^{(l-1)} = \\frac{\\partial J }{\\partial \\textbf{A}^{(l-1)}} = \\textbf{W}^{(l) T} d\\textbf{Z}^{(l)} = \\textbf{W}^{(l) T} \\textbf{E}^{(l)} \\tag{10}$$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QaXb8X5maclW"},"source":["**TODO 2:** Lập trình các công thức trên cho hàm lan truyền ngược tuyến tính"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Qp5rYoquPDO"},"outputs":[],"source":["def linear_backward(E, cache):\n","    \"\"\"\n","    Tính toán đạo hàm của hàm mất mát trên A, W và b\n","    Đầu vào:\n","      E hoặc dZ:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với đầu ra của phép tuyến tính tại lớp hiện tại (l)\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","      cache\n","        Dạng: Python tuple\n","        Miêu tả: Biến lưu trữ (cache) của lớp linear\n","        Bao gồm các biến A_prev, W và b\n","    Đầu ra:\n","      dA_prev:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với đầu ra phép phi tuyến của lớp l-1\n","        Chiều: (Chiều của lớp phía trước, Số lượng điểm dữ liệu)\n","      dW:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với tham số của lớp hiện tại (l)\n","        Chiều: (Chiều của lớp hiện tại, Chiều của lớp phía trước)\n","      db:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với bias của lớp hiện tại (l)\n","        Chiều: (Chiều của lớp hiện tại, 1)\n","    \"\"\"\n","    # Lập trình tại đây\n","\n","    A_prev, W, b = cache\n","\n","    m = A_prev.shape[1]\n","\n","    # Tính gradient trên W\n","    dW = 1/m * np.dot(E, A_prev.T)\n","\n","    # Tính gradient trên b\n","    db = 1/m * np.sum(E, keepdims=True, axis=1)\n","\n","    # Tính gradient trên A phía trước\n","    dA_prev = np.dot(W.T, E)\n","\n","    return dA_prev, dW, db"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1698999876528,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"eWjslJ75uPDS","outputId":"5848dbd8-88a1-4fa4-bc25-4a3a3b53086a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Chiều của dA1: (256, 60000)\n","Chiều của dW2: (128, 256)\n","Chiều của db2: (128, 1)\n","Giá trị dA1 được tính đúng?: True\n","Giá trị dW2 được tính đúng?: True\n","Giá trị db2 được tính đúng?: True\n"]}],"source":["try:\n","  # Test trên layer 2\n","  E2 = dZ = np.ones((128, 60000))\n","  A1 = np.ones((256, 60000))\n","  W2 = np.ones((128, 256))\n","  b2 = np.ones((128,1))\n","  linear_cache = (\n","      A1, W2, b2\n","  )\n","\n","  dA1, dW2, db2 = linear_backward(E2, linear_cache)\n","  print('Chiều của dA1: {}'.format(dA1.shape))\n","  print('Chiều của dW2: {}'.format(dW2.shape))\n","  print('Chiều của db2: {}'.format(db2.shape))\n","  print('Giá trị dA1 được tính đúng?: {}'.format(np.array_equal(np.full(A1.shape, fill_value=128.), dA1)))\n","  print('Giá trị dW2 được tính đúng?: {}'.format(np.array_equal(np.full(W2.shape, fill_value=1.), dW2)))\n","  print('Giá trị db2 được tính đúng?: {}'.format(np.array_equal(np.full(b2.shape, fill_value=1.), db2)))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"JBV-QJffuPDU"},"source":["**Kết quả mong đợi**:\n","\n","```\n","Chiều của dA1: (256, 60000)\n","Chiều của dW2: (128, 256)\n","Chiều của db2: (128, 1)\n","Giá trị dA1 được tính đúng?: True\n","Giá trị dW2 được tính đúng?: True\n","Giá trị db2 được tính đúng?: True\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"_Jpgrx_OfUhY"},"source":["### 6.2 - Lan truyền ngược phi tuyến (Activation backward)"]},{"cell_type":"markdown","metadata":{"id":"WuK2em4AfWe6"},"source":["**TODO 3**: Lập trình đạo hàm của RELU"]},{"cell_type":"markdown","metadata":{"id":"QEbW0Qh0uPDU"},"source":["\n","\n","\n","Nếu $f(.)$  là activation function `relu_backward` được tính: $$\\textbf{E}^{(l)} = d\\textbf{Z}^{(l)} = d\\textbf{A}^{(l)} * f'(\\textbf{Z}^{(l)})$$\n","\n","- Với **RELU**\n","$$f'(z) = \\left\\{\\begin{matrix}\n","0 \\ \\ \\forall z < 0 \\\\\n","1 \\ \\ \\forall z > 0 \\\\\n","DNE \\ \\ \\text{if} \\ \\ z = 0 \\\\\n","\\end{matrix}\\right.$$\n","\n","**RELU** `không có đạo` hàm tại 0 tuy nhiên khi lập trình ta sẽ cài đặt sẵn với trường hơp $z=0$ đạo hàm của RELU sẽ bằng `0`. Vậy công thức trở thành.\n","\n","$$f'(z) = \\left\\{\\begin{matrix}\n","0 \\ \\ \\forall z <= 0 \\\\\n","1 \\ \\ \\forall z > 0 \\\\\n","\\end{matrix}\\right.$$\n","\n","Cùng lập trình đạo hàm này với trình tự:\n","\n","- Tất cả những giá trị của Z nhỏ hơn hoặc bằng 0 sẽ được đặt thành 0\n","- Tất cả những giá trị của Z lớn hơn 0 sẽ được đặt thành 1"]},{"cell_type":"markdown","metadata":{"id":"0CR7gSLyDE7G"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4mvUgNJjOzk"},"outputs":[],"source":["def relu_derivative(Z):\n","  \"\"\"\n","  Thực hiện đạo hàm của RELU trên ma trận Z\n","  Đầu vào:\n","    Z:\n","      Dạng: numpy array\n","      Miêu tả: Đầu ra của phép nhân tuyến tính và cũng là đầu vào của hàm activation\n","      Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","  Đầu ra:\n","    Z_grad:\n","      Dạng: numpy array\n","      Miêu tả: Đạo hàm của hàm RELU với Z\n","      Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","  \"\"\"\n","  # Lập trình tại đây\n","\n","  Z_grad = np.array(Z, copy=True)\n","\n","  Z_grad[Z <= 0] = 0\n","  Z_grad[Z > 0] = 1\n","\n","  return Z_grad\n"]},{"cell_type":"markdown","metadata":{"id":"hdPfw7FPksJc"},"source":["Test code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1006,"status":"ok","timestamp":1698999877530,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"xNb9JCDSktI2","outputId":"2ccfc7fe-21b6-40e2-b1d9-be08ccb5c7d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Chiều của đạo hàm hàm RELU trên Z1: (128, 60000)\n","Chiều của đạo hàm hàm RELU trên Z2: (128, 60000)\n","Giá trị đạo hàm hàm RELU trên Z1 đã được tính đúng?: True\n","Giá trị đạo hàm hàm RELU trên Z2 đã được tính đúng?: True\n"]}],"source":["try:\n","  # Test trên layer 2\n","  Z1 = np.full((128, 60000), 2.)\n","  Z2 = np.full((128, 60000), -2.)\n","\n","  Z1_derivative = relu_derivative(Z1)\n","  Z2_derivative = relu_derivative(Z2)\n","\n","\n","  print('Chiều của đạo hàm hàm RELU trên Z1: {}'.format(Z1_derivative.shape))\n","  print('Chiều của đạo hàm hàm RELU trên Z2: {}'.format(Z2_derivative.shape))\n","  print('Giá trị đạo hàm hàm RELU trên Z1 đã được tính đúng?: {}'.format(np.array_equal(Z1_derivative, np.ones(Z1.shape))))\n","  print('Giá trị đạo hàm hàm RELU trên Z2 đã được tính đúng?: {}'.format(np.array_equal(Z2_derivative, np.zeros(Z2.shape))))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"S4uwd6jtmtK5"},"source":["**Kết quả mong đợi**:\n","\n","```\n","Chiều của đạo hàm hàm RELU trên Z1: (128, 60000)\n","Chiều của đạo hàm hàm RELU trên Z2: (128, 60000)\n","Giá trị đạo hàm hàm RELU trên Z1 đã được tính đúng?: True\n","Giá trị đạo hàm hàm RELU trên Z2 đã được tính đúng?: True\n","```"]},{"cell_type":"markdown","metadata":{"id":"_zNpCYprm3K2"},"source":["**TODO 4**: Lập trình hàm lan truyền ngược trên hàm phi tuyến\n","\n","Cụ thể là đạo hàm của **hàm mất mát** trên giá trị đầu vào hàm phi tuyến hay nói cách khác chính là:\n","\n","$$\\textbf{E}^{(l)} = d\\textbf{Z}^{(l)} = \\frac{\\partial J }{\\partial \\textbf{Z}^{(l)}} = d\\textbf{A}^{(l)} * f'(\\textbf{Z}^{(l)})$$"]},{"cell_type":"markdown","metadata":{"id":"a86EtnOZSx2k"},"source":["<img src=\"https://storage.googleapis.com/protonx-cloud-storage/backprop5.PNG\">"]},{"cell_type":"markdown","metadata":{"id":"KhPqOTFOnZPk"},"source":["Cho nên đầu vào của hàm tính giá trị $\\textbf{E}^{(l)}$ sẽ bao gồm 2 giá trị:\n","- Đạo hàm của hàm mất mát với đầu ra phép phi tuyến ở lớp hiện tại đã được tính trước: $d\\textbf{A}^{(l)}$\n","- Giá trị $\\textbf{Z}$ đã lưu lại khi thực hiện phi tuyến theo chiều thuận\n","\n","Hàm này sẽ thực hiện nhiệm vụ:\n","\n","- Sử dụng hàm `relu_derivative` để tính đạo hàm $f'(\\textbf{Z}^{(l)})$ của hàm RELU với $\\textbf{Z}$\n","- Sau đó lấy đầu vào $d\\textbf{A}^{(l)}$ nhân Hadamard với giá trị tìm được"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgi9ZEUDpz9E"},"outputs":[],"source":["def relu_backward(dA, cache):\n","    \"\"\"\n","    Thực hiện lan truyền ngược qua hàm phi tuyến RELU\n","    Hay nói cách khác đạo hàm của hàm mất mát trên Z\n","    Đầu vào:\n","      dA:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với đầu ra phép phi tuyến của lớp l\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","      cache:\n","        Dạng: numpy array\n","        Miêu tả: Z được lưu lại từ hàm linear-activation forward\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","    Đầu ra:\n","      E (dZ):\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của giá trị mất mát với Z\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","    \"\"\"\n","    # Lập trình tại đây\n","\n","    Z = cache\n","\n","    # Sử dụng hàm relu_derivative\n","    Z_derivative = relu_derivative(Z)\n","\n","    # Áp dụng công thức để tính E\n","    E = dZ = dA * Z_derivative\n","\n","    return E"]},{"cell_type":"markdown","metadata":{"id":"mLAd4St6TPYY"},"source":["Test Code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1698999877531,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"vu4-QHRrTRCw","outputId":"64816da5-45ac-4262-e21d-1528919cd615"},"outputs":[{"output_type":"stream","name":"stdout","text":["Chiều của E1: (128, 60000)\n","Chiều của E2: (128, 60000)\n","Giá trị E1 đã được tính đúng?: True\n","Giá trị E2 đã được tính đúng?: True\n"]}],"source":["try:\n","  dA1 = np.full((128, 60000), -2.)\n","  cache1 = np.full((128, 60000), -3.)\n","  E1 = relu_backward(dA1, cache1)\n","\n","  dA2 = np.full((128, 60000), 1.)\n","  cache2 = np.full((128, 60000), 1.)\n","  E2 = relu_backward(dA2, cache2)\n","\n","  print('Chiều của E1: {}'.format(E1.shape))\n","  print('Chiều của E2: {}'.format(E2.shape))\n","  print('Giá trị E1 đã được tính đúng?: {}'.format(np.array_equal(E1, np.full(E1.shape, 0.))))\n","  print('Giá trị E2 đã được tính đúng?: {}'.format(np.array_equal(E2, np.full(E2.shape, 1.))))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"BxeqogguVgv-"},"source":["**Kết quả mong đợi**:\n","\n","```\n","Chiều của E1: (128, 60000)\n","Chiều của E2: (128, 60000)\n","Giá trị E1 đã được tính đúng?: True\n","Giá trị E2 đã được tính đúng?: True\n","```"]},{"cell_type":"markdown","metadata":{"id":"bcKXV9JwVrwD"},"source":["### 6.3 Lan truyền trên một lớp (Linear-Activation backward)"]},{"cell_type":"markdown","metadata":{"id":"qq68t8InVzYM"},"source":["**TODO 5:** Kết hợp phần `6.1` và `6.2` để tạo thành một hàm đầy đủ việc lan truyền ngược trên một lớp ẩn bất kỳ. Chú ý **đường màu xanh**."]},{"cell_type":"markdown","metadata":{"id":"detEtiDAc5rQ"},"source":["<img src=\"https://storage.googleapis.com/protonx-cloud-storage/Backprop7.PNG\" >"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rc4-K95KuPDV"},"outputs":[],"source":["def linear_activation_backward(dA, cache, activation):\n","    \"\"\"\n","    Thực hiện lan truyền ngược trên lớp này\n","    Sử dụng kết hợp relu_backward và hàm linear_backward\n","    Đầu vào:\n","      dA:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với đầu ra phép phi tuyến của lớp l\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","      cache:\n","        Dạng: Python tuple\n","        Miêu tả: (linear_cache, activation_cache)\n","        Bao gồm:\n","          linear_cache:\n","            Dạng: Python tuple\n","            Miêu tả: cache lưu lại từ phép tuyến tính thuận: (A_pre, W, b)\n","          activation_cache:\n","            Dạng: Numpy array\n","            Miêu tả: cache lưu lại từ phép phi tính thuận: (Z)\n","      activation:\n","        Dạng: Python string\n","        Miêu tả: Tên của activation\n","        Ví dụ: \"relu\"\n","    Đầu ra:\n","      dA_prev:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với đầu ra phép phi tuyến của lớp l-1\n","        Chiều: (Chiều của lớp phía trước, Số lượng điểm dữ liệu)\n","      dW:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với tham số của lớp hiện tại (l)\n","        Chiều: (Chiều của lớp hiện tại, Chiều của lớp phía trước)\n","      db:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với bias của lớp hiện tại (l)\n","        Chiều: (Chiều của lớp hiện tại, 1)\n","    \"\"\"\n","    # Lập trình tại đây\n","\n","    linear_cache, activation_cache = cache\n","\n","    # Thực hiện activation backward\n","    # Từ dA và activation_cache để tính ra dZ\n","    if activation == \"relu\":\n","        dZ = relu_backward(dA, activation_cache)\n","\n","    # Hàm này có thể thiết kế để sử dụng nhiều hàm khác nhau trong tương lai\n","    # Bạn không cần code elif này\n","    elif activation == \"gelu\":\n","        pass\n","\n","    # Thực hiện linear backward\n","    # Từ dZ và linear_cache để tính ra dA_prev, dW và db\n","    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","\n","    return dA_prev, dW, db"]},{"cell_type":"markdown","metadata":{"id":"oaOh_-h9Wxat"},"source":["Test code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1698999877532,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"5aOjEaskWzKN","outputId":"c6a52a12-63af-418a-e6fb-ccdaa06b353e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Chiều của dA1: (256, 60000)\n","Chiều của dW: (128, 256)\n","Chiều của db: (128, 1)\n","Giá trị dA1 đã được tính đúng?: True\n","Giá trị dW đã được tính đúng?: True\n","Giá trị db đã được tính đúng?: True\n"]}],"source":["try:\n","  np.random.seed(42)\n","  dA2 = np.full((128, 60000), 4.)\n","  activation = 'relu'\n","  A1 = np.ones((256, 60000))\n","  W2 = np.full((128, 256), 2.)\n","  b2 = np.ones((128,1))\n","  linear_cache = (\n","      A1, W2, b2\n","  )\n","  activation_cache = Z = np.full((128, 60000), 2.5)\n","  cache = (linear_cache, activation_cache)\n","\n","  dA1, dW, db = linear_activation_backward(dA2, cache, activation)\n","\n","\n","  print('Chiều của dA1: {}'.format(dA1.shape))\n","  print('Chiều của dW: {}'.format(dW.shape))\n","  print('Chiều của db: {}'.format(db.shape))\n","  print('Giá trị dA1 đã được tính đúng?: {}'.format(np.array_equal(dA1, np.full(dA1.shape, 1024.))))\n","  print('Giá trị dW đã được tính đúng?: {}'.format(np.array_equal(dW, np.full(W2.shape, 4.))))\n","  print('Giá trị db đã được tính đúng?: {}'.format(np.array_equal(E2, np.full(E2.shape, 1.))))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"CzcH7WU1Qrk6"},"source":["**Kết quả mong đợi**:\n","\n","```\n","Chiều của dA1: (256, 60000)\n","Chiều của dW: (128, 256)\n","Chiều của db: (128, 1)\n","Giá trị dA1 đã được tính đúng?: True\n","Giá trị dW đã được tính đúng?: True\n","Giá trị db đã được tính đúng?: True\n","```"]},{"cell_type":"markdown","metadata":{"id":"lHXPbi_juPDe"},"source":["### 6.4 - Cập nhật tham số\n","\n","Trong phần này, bạn sẽ thực hiện cập nhật các tham số:\n","\n","\n","$$ \\textbf{W}^{(l)} = \\textbf{W}^{(l)} - \\alpha \\text{ } d\\textbf{W}^{(l)} \\tag{16}$$\n","$$ \\textbf{b}^{(l)} = \\textbf{b}^{(l)} - \\alpha \\text{ } d\\textbf{b}^{(l)} \\tag{17}$$\n","\n","trong đó $\\alpha$ là learning rate. Sau khi tính cập nhật giá trị Parameters, chúng ta sẽ lưu trữ trong trong 1 parameters dictionary."]},{"cell_type":"markdown","metadata":{"id":"RmDVbzIbuPDe"},"source":["**TODO 6**: Thực hiện `update_parameters()`.\n","\n","**Hướng dẫn**: Cập nhật tham số sử dụng Gradient Descent cho $\\textbf{W}^{(l)}$ và $\\textbf{b}^{(l)}$ của từng layer $l = 1, 2, ..., L$.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aug2qIkVuPDe"},"outputs":[],"source":["def update_parameters(parameters, grads, learning_rate):\n","    \"\"\"\n","    Lặp qua các lớp và cập nhật tham số sử dụng Gradient Descent\n","    Đầu vào:\n","      parameters:\n","        Dạng: Python dictionary\n","        Miêu tả: Chứa các tham số với key là tên và giá trị là numpy array tham số đó\n","        Ví dụ: {\n","          W1: ...,\n","          b1: ...,\n","          ...\n","        }\n","      grads:\n","        Dạng: Python dictionary\n","        Miêu tả: Chứa gradient của các tham số với key là tên gradient và giá trị là numpy array của gradient\n","        Ví dụ: {\n","          dW1: ...,\n","          db1: ...,\n","          ...\n","        }\n","    Đầu ra:\n","      parameters:\n","        Dạng: Python dictionary\n","        Miêu tả: Chứa các tham số đã được cập nhật gradient\n","        Ví dụ: {\n","          W1: ...,\n","          b1: ...,\n","          ...\n","        }\n","    \"\"\"\n","    # Lập trình tại đây\n","\n","    # Tính số lượng lớp\n","    L = len(parameters) // 2\n","\n","    # Lặp qua các lớp và cập nhật tham số W và b\n","    for l in range(L):\n","        parameters[\"W\" + str(l + 1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n","        parameters[\"b\" + str(l + 1)] -= learning_rate * grads[\"db\" + str(l+1)]\n","\n","    return parameters"]},{"cell_type":"markdown","metadata":{"id":"KVt3Fqnl7UhJ"},"source":["Test code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1698999877532,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"WBlqTwWD7XH5","outputId":"48565c5a-aaaf-4c21-d21a-16b050060dcb"},"outputs":[{"output_type":"stream","name":"stdout","text":["W1 được cập nhật đúng?: True\n","b1 được cập nhật đúng?: True\n","W2 được cập nhật đúng?: True\n","b2 được cập nhật đúng?: True\n","W3 được cập nhật đúng?: True\n","b3 được cập nhật đúng?: True\n"]}],"source":["try:\n","  n = 784\n","  d_1 = 128\n","  d_2 = 256\n","  d_3 = 1028\n","  np.random.seed(42)\n","  parameters = {\n","      \"W1\": np.random.randn(d_1, n),\n","      \"b1\": np.zeros((d_1, 1)),\n","      \"W2\": np.random.randn(d_2, d_1),\n","      \"b2\": np.zeros((d_2, 1)),\n","      \"W3\": np.random.randn(d_3, d_2),\n","      \"b3\": np.zeros((d_3, 1)),\n","  }\n","\n","  grads = {\n","      \"dW1\": np.full((d_1, n), -0.02),\n","      \"db1\": np.full((d_1, 1), 0.1),\n","      \"dW2\": np.full((d_2, d_1), -0.4),\n","      \"db2\": np.full((d_2, 1), 0.5),\n","      \"dW3\": np.full((d_3, d_2), 0.6),\n","      \"db3\": np.full((d_3, 1), -0.02),\n","  }\n","\n","  lr = 0.01\n","  updated_parameters = update_parameters(parameters, grads, lr)\n","  exp = {'W1': 106.015, 'b1': -0.128, 'W2': 200.643, 'b2': -1.28, 'W3': -1661.189, 'b3': 0.206}\n","  for key in updated_parameters:\n","    print(\"{} được cập nhật đúng?: {}\".format(key, exp[key] == np.round(np.sum(updated_parameters[key]), 3)))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"7Y-BfyHGQt2S"},"source":["**Kết quả mong đợi**:\n","\n","```\n","W1 được cập nhật đúng?: True\n","b1 được cập nhật đúng?: True\n","W2 được cập nhật đúng?: True\n","b2 được cập nhật đúng?: True\n","W3 được cập nhật đúng?: True\n","b3 được cập nhật đúng?: True\n","```"]},{"cell_type":"markdown","metadata":{"id":"anM9Ho3fMPVo"},"source":["##  7 - Train Model"]},{"cell_type":"markdown","metadata":{"id":"osy4dRRiRa1E"},"source":["### 7.1. Tải dữ liệu MNIST"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4905,"status":"ok","timestamp":1698999882432,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"iVlUEU7pceyr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"30a8bd12-e6df-4bb5-ff25-b79c0bc528f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 1s 0us/step\n"]}],"source":["import tensorflow as tf\n","(X_train, Y_train), (X_val, Y_val) = tf.keras.datasets.mnist.load_data()"]},{"cell_type":"markdown","metadata":{"id":"e2mD77mIutDs"},"source":["Một số thông tin quan trọng"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1698999882432,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"vzRTUc-cuu_6","outputId":"d25fb1a5-06e6-4be8-b274-9692df3abbc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","  Số lượng ảnh train: 60000\n","  Chiều dài ảnh train: 28\n","  Chiều cao ảnh train: 28\n","  Chiều ảnh được duỗi: 784\n","\n"]}],"source":["# Số lượng nhãn\n","num_classes = 10\n","\n","num_of_train_images, width, height = X_train.shape\n","\n","# Chiều ảnh được duỗi\n","image_vector_size = width * height\n","\n","\n","print(\"\"\"\n","  Số lượng ảnh train: {}\n","  Chiều dài ảnh train: {}\n","  Chiều cao ảnh train: {}\n","  Chiều ảnh được duỗi: {}\n","\"\"\".format(num_of_train_images, width, height, image_vector_size))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"egzxTzYNRfEV"},"source":["### 7.2. Tiền xử lý dữ liệu"]},{"cell_type":"markdown","metadata":{"id":"Is0v-oBj0b8t"},"source":["Các hàm này được sử dụng từ bài Softmax Regression, nếu bạn chưa hiểu kỹ bài tập này thì hãy làm bài tập đó trước nhé ;)"]},{"cell_type":"code","source":["np.eye(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AwyBNz8W1--p","executionInfo":{"status":"ok","timestamp":1698999882432,"user_tz":-420,"elapsed":3,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"}},"outputId":"67660cc8-4fcc-4323-fc78-c5ae4087b158"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FE1ZkXaoWgIl"},"outputs":[],"source":["def one_hot(y, num_classes):\n","  \"\"\"\n","  Đầu vào:\n","    y:\n","      Dạng: numpy array\n","      Miêu tả: Các giá trị nhãn\n","      Chiều: (batch_size,)\n","    num_classes:\n","      Dạng: Python integer\n","      Miêu tả: Số lượng nhãn\n","      Điều kiện: num_classes > 0\n","      Ví dụ: 10\n","  Đầu ra:\n","    y_one_hot:\n","      Miêu tả: Trả về ma trận các vector one hot của từng nhãn\n","      Chiều: (batch_size, num_classes)\n","  \"\"\"\n","  y_one_hot = np.squeeze(np.eye(num_classes)[y.reshape(-1)])\n","\n","  return y_one_hot\n","\n","def flatten_images(images):\n","  \"\"\"\n","  Thực thi duỗi ảnh từ (batch_size, width, height) thành (batch_size, width x height)\n","  Đầu vào:\n","    images:\n","      Dạng: numpy array\n","      Miêu tả: Ma trận các ảnh\n","      Chiều: (batch_size, width, height)\n","      Ví dụ: (32, 28, 28)\n","  Đầu ra:\n","    flattened_images\n","      Miêu tả: ma trận trong đó các ảnh được duỗi thành vector\n","      Chiều: (batch_size, image_vector_size) = (batch_size, width x height)\n","      Ví dụ: (32, 28 x 28) = (32, 784)\n","  \"\"\"\n","  flattened_images = np.reshape(images, (images.shape[0], -1))\n","\n","  return flattened_images\n","\n","def normalize_images(images):\n","  \"\"\"\n","  Hàm chuẩn hóa ảnh các giá trị pixel để nằm trong khoảng từ 0 đến 1\n","  Đầu vào:\n","    images:\n","      Dạng: numpy array\n","      Miêu tả: Ma trận các vector ảnh\n","      Chiều: (batch_size, image_vector_size)\n","      Ví dụ: (32, 784)\n","  Đầu ra:\n","    normailized_images\n","      Miêu tả: ma trận trong đó các vector ảnh được chuẩn hóa\n","      Chiều: (batch_size, image_vector_size)\n","      Ví dụ: (32, 784)\n","  \"\"\"\n","  normailized_images = images / 255.0\n","\n","  return normailized_images\n","\n","try:\n","  X_train = flatten_images(X_train)\n","  X_val = flatten_images(X_val)\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)\n","\n","try:\n","  X_train = normalize_images(X_train)\n","  X_val = normalize_images(X_val)\n","except Exception as e:\n","   print(\"Lỗi thực thi: \", e)\n","\n","# Chuyển nhãn thành one hot\n","# y_train = one_hot(Y_train, 10)\n"]},{"cell_type":"markdown","metadata":{"id":"AqztuH9zeTOe"},"source":["### 7.3. Định nghĩa chiều các lớp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4btCQmPABl0B"},"outputs":[],"source":["n = image_vector_size\n","d_1 = 1024\n","d_2 = 256\n","c = 10\n","\n","layers_dims = (n, d_1, d_2, c)"]},{"cell_type":"markdown","metadata":{"id":"29Y2QRPqtPVH"},"source":["### 7.4. Xây dựng mô hình"]},{"cell_type":"markdown","metadata":{"id":"jxhPHl7metoN"},"source":["**TODO 7**: Lập trình hàm tính độ chính xác"]},{"cell_type":"markdown","metadata":{"id":"2yNLprEw0qoT"},"source":["Trong trường hợp này trong thân hàm cần chú ý tới chiều\n","\n","\n","- $n: $ chiều vector ảnh (784)\n","- $m: $ số ảnh (60000)\n","- $c: $ số nhãn (10)\n","\n","Chiều của $\\textbf{X} \\in \\mathbf{R} ^ {n \\times m}$\n","Chiều của $\\textbf{Y} \\in \\mathbf{R} ^ {m \\times 1}$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJHHg064vgUa"},"outputs":[],"source":["def cal_acc(parameters, X, Y):\n","  \"\"\"\n","  Tính độ chính xác của mô hình trên toàn bộ tập dữ liệu\n","    Bước 1: Thực hiện feed forward để lấy kết quả dự đoán\n","    Bước 2: So sánh kết quả dự đoán với nhãn thật\n","  Đầu vào:\n","    X:\n","      Dạng: numpy array\n","      Miêu tả: Các vector ảnh\n","      Chiều: (image_vector_size, Số lượng điểm dữ liệu)\n","      Ví dụ: (784, 60000)\n","    Y:\n","      Dạng: numpy arry\n","      Miêu tả: Vector của nhãn\n","      Chiều: (Số lượng điểm dữ liệu,)\n","      Ví dụ: (60000,)\n","  Đầu ra:\n","    acc:\n","      Dạng: số thực\n","      Miêu tả: Độ chính xác của mô hình tại thời điểm hiện tại\n","      Ví dụ: 0.6\n","  \"\"\"\n","  # Lập trình tại đây\n","\n","  W1 = parameters[\"W1\"]\n","  b1 = parameters[\"b1\"]\n","  W2 = parameters[\"W2\"]\n","  b2 = parameters[\"b2\"]\n","  W3 = parameters[\"W3\"]\n","  b3 = parameters[\"b3\"]\n","\n","  A1, cache1 = linear_activation_forward(X, W1, b1, 'relu')\n","  A2, cache2 = linear_activation_forward(A1, W2, b2, 'relu')\n","  A3, cache3 = linear_activation_forward(A2, W3, b3, 'softmax')\n","\n","  # (Số lượng điểm dữ liệu, số lượng nhãn)\n","  Y_hat = A3.T\n","  acc = np.sum(np.equal(np.argmax(Y_hat, axis=1), Y).astype(float)) / Y.shape[0]\n","\n","  return acc"]},{"cell_type":"markdown","metadata":{"id":"Rg0DUEp_OpZv"},"source":["Test code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1698999883285,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"Fhz9UoQzOsWo","outputId":"ed0dfbd1-6b8a-4c4b-fd89-bdf5144b69b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["(10, 4) (4,)\n","Độ chính xác: 0.500\n"]}],"source":["try:\n","  np.random.seed(42)\n","  parameters = {\n","    \"W1\": np.random.randn(d_1, n),\n","    \"b1\": np.zeros((d_1, 1)),\n","    \"W2\": np.random.randn(d_2, d_1),\n","    \"b2\": np.zeros((d_2, 1)),\n","    \"W3\": np.random.randn(num_classes, d_2),\n","    \"b3\": np.zeros((num_classes, 1)),\n","  }\n","\n","  Y_mock =  np.array([5,2,6, 4])\n","  X_mock = X_mock = X_train[:4].T / 255.0\n","  acc_mock = cal_acc(parameters, X_mock, Y_mock)\n","  print(\"Độ chính xác: {:.3f}\".format(acc_mock))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"MzJuCLa9Q2-2"},"source":["**Kết quả mong đợi**:\n","\n","```\n","Độ chính xác: 0.500\n","```"]},{"cell_type":"markdown","metadata":{"id":"NqfxKw9xOspa"},"source":["**TODO 8** Xây dựng mô hình và tiến hành training\n"]},{"cell_type":"markdown","metadata":{"id":"YT0oevf9qxtN"},"source":["<img src=\"https://storage.googleapis.com/protonx-cloud-storage/images/Backprop4.PNG\"  />"]},{"cell_type":"markdown","metadata":{"id":"rEs4jQG9e0Mm"},"source":["Với 2 lớp ta sẽ có các công thức sau.\n","\n","\n","**Chiều Backward:**\n","\n","- Lớp Softmax cuối cùng:\n","\n","$$\\textbf{E}^{(3)} = \\hat{\\textbf{Y}} - \\textbf{Y} $$\n","\n","$$ d\\textbf{A}^{(2)} = \\textbf{W}^{(3)T}\\textbf{E}^{(3)} $$\n","\n","\n","$$ d\\textbf{W}^{(3)} = \\frac{1}{m} \\textbf{E}^{(3)}\\textbf{A}^{(2)T} \\rightarrow \\textbf{W}^{(3)} := \\textbf{W}^{(3)}  - \\alpha \\frac{1}{m} \\textbf{E}^{(3)}\\textbf{A}^{(2)T}    $$\n","\n","$$ d\\textbf{b}^{(3)} = \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(3)(i)}\\rightarrow \\textbf{b}^{(3)} := \\textbf{b}^{(3)}  - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(3)(i)}$$\n","\n","- Lớp ẩn số 2:\n","\n","$$\\textbf{E}^{(2)} = d\\textbf{Z}^{(2)} = {d\\textbf{A}^{(2)}} \\odot f'(\\textbf{Z}^{(2)}) $$\n","\n","$$ d\\textbf{A}^{(1)} = \\textbf{W}^{(2)T}\\textbf{E}^{(2)} $$\n","\n","$$ d\\textbf{W}^{(2)} = \\frac{1}{m} \\textbf{E}^{(2)}\\textbf{A}^{(1)T} \\rightarrow \\textbf{W}^{(2)} := \\textbf{W}^{(2)}  - \\alpha \\frac{1}{m} \\textbf{E}^{(2)}\\textbf{A}^{(1)T}    $$\n","\n","$$ d\\textbf{b}^{(2)} = \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(2)(i)}\\rightarrow \\textbf{b}^{(2)} := \\textbf{b}^{(2)}  - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(2)(i)}$$\n","\n","\n","- Lớp ẩn số 1:\n","\n","$$\\textbf{E}^{(1)} = d\\textbf{Z}^{(1)} = {d\\textbf{A}^{(1)}} \\odot f'(\\textbf{Z}^{(1)}) $$\n","\n","$$ d\\textbf{W}^{(1)} = \\frac{1}{m} \\textbf{E}^{(1)}\\textbf{X}^{T} \\rightarrow \\textbf{W}^{(1)} := \\textbf{W}^{(1)}  - \\alpha \\frac{1}{m} \\textbf{E}^{(1)}\\textbf{X}^{T}    $$\n","\n","$$ d\\textbf{b}^{(1)} = \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(1)(i)}\\rightarrow \\textbf{b}^{(1)} := \\textbf{b}^{(1)}  - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(1)(i)}$$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lt1hTST0Kc-F"},"source":["Nếu bạn thắc mắc tại sao $$\\textbf{E}^{(3)} = \\hat{\\textbf{Y}} - \\textbf{Y} $$\n","\n","thì lời giải nằm [tại đây](https://colab.research.google.com/drive/1awivtxQzjgNK2m-cEQLIyzE5g6MxcZpX#scrollTo=NYrcc6YWGz1S)."]},{"cell_type":"markdown","metadata":{"id":"yi2yxBGsTdHR"},"source":["**TODO 9:** Xây dựng hàm train model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OnePW1bgNNv0"},"outputs":[],"source":["def train_model(X_train, Y_train, X_val, Y_val, layers_dims, learning_rate=0.1, epochs=250):\n","    \"\"\"\n","    Hàm xây dựng và train model\n","      Bước 1: Thực hiện lan truyền thuận, tính giá trị đầu ra của các lớp\n","      Bước 2: Tính giá trị mất mát\n","      Bước 3: Thực hiện lan truyền ngược, tính giá trị đạo hàm của hàm mất mát trên các tham số\n","      Bước 4: Cập nhật tham số của mô hình\n","    Đầu vào:\n","      X_train:\n","        Dạng: numpy array\n","        Miêu tả: Các vector ảnh\n","        Chiều: (Số lượng điểm dữ liệu, image_vector_size)\n","        Ví dụ: (60000, 784)\n","      Y_train:\n","        Dạng: numpy arry\n","        Miêu tả: Vector của nhãn\n","        Chiều: (Số lượng điểm dữ liệu,)\n","        Ví dụ: (60000,)\n","      X_val:\n","        Dạng: numpy array\n","        Miêu tả: Các vector ảnh\n","        Chiều: (Số lượng điểm dữ liệu, image_vector_size)\n","        Ví dụ: (10000, 784)\n","      Y_val:\n","        Dạng: numpy arry\n","        Miêu tả: Vector của nhãn\n","        Chiều: (Số lượng điểm dữ liệu,)\n","        Ví dụ: (10000,)\n","      layers_dims:\n","        Dạng: Python tuple\n","        Miêu tả: (n, d_1, d_2, num_classes)\n","          n: Chiều của vector ảnh\n","          d_1: Chiều của lớp ẩn 1\n","          d_2: Chiều của lớp ẩn 2\n","          num_classes: số nhãn\n","      learning_rate:\n","        Dạng: Python float\n","        Miêu tả: Tốc độ học của mô hình\n","        Ví dụ: 0.001\n","      epochs:\n","        Dạng: Python integer\n","        Miêu tả: Số vòng lặp qua tập dữ liệu\n","        Ví dụ: 100\n","    Đầu ra:\n","      Python Tuple: (parameters, acc, val_acc, costs)\n","      parameters:\n","        Dạng: Python Dictionary\n","        Miêu tả: Bộ tham số của mô hình\n","      acc:\n","        Dạng: số thực\n","        Miêu tả: Độ chính xác của mô hình trên bộ train\n","        Ví dụ: 0.6\n","      val_acc:\n","        Dạng: số thực\n","        Miêu tả: Độ chính xác của mô hình trên bộ validation\n","        Ví dụ: 0.7\n","      costs:\n","        Dạng: Python List\n","        Miêu tả: Giá trị mất mát trên từng epoch\n","        Ví dụ [0.2, 0.15, 0.1]\n","    \"\"\"\n","\n","    np.random.seed(42)\n","    grads = {}\n","    costs = []\n","\n","    m = X_train.shape[0]\n","\n","    (n, d_1, d_2, num_classes) = layers_dims\n","\n","    # 0. Khởi tạo tham số\n","    parameters = initialize_parameters(n, d_1, d_2, num_classes)\n","\n","    # 1. Điều chỉnh chiều dữ liệu sao cho\n","    # X_train: (image_vector_size, Số lượng điểm dữ liệu)\n","    # Y_train_one_hot: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","    # X_val: (image_vector_size, Số lượng điểm dữ liệu)\n","\n","    X_train = X_train.T\n","    Y_train_one_hot = one_hot(Y_train, num_classes).T\n","    X_val = X_val.T\n","\n","\n","    # 2. Lấy các tham số ra khỏi dictionary: parameters\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","    W3 = parameters[\"W3\"]\n","    b3 = parameters[\"b3\"]\n","\n","    # 3. Lấy các tham số ra khỏi dictionary: parameters\n","    for i in range(0, epochs):\n","\n","        # 3.1. Thực hiện Feed Forward trên 3 lớp: 2 lớp relu và lớp Softmax\n","        A1, cache1 = linear_activation_forward(X_train, W1, b1, 'relu')\n","        A2, cache2 = linear_activation_forward(A1, W2, b2, 'relu')\n","        A3, cache3 = linear_activation_forward(A2, W3, b3, 'softmax')\n","\n","        Y_hat = A3\n","\n","        # 3.2. Tính giá trị mất mát\n","        cost = compute_cost(Y_hat, Y_train_one_hot)\n","\n","        # 3.3. Tính giá trị E3\n","        E3 = Y_hat - Y_train_one_hot\n","\n","        # 3.4. Tính giá trị dA2\n","        dA2 = W3.T.dot(E3)\n","\n","        # 3.5. Tính gía trị dW3\n","        dW3 = 1/m * np.dot(E3, A2.T)\n","\n","        # 3.6. Tính giá trị db3\n","        # db3 = 1/m * np.sum(E3) # <---- Bug\n","        # Fix:\n","        db3 = 1/m * np.sum(E3, keepdims=True, axis=1)\n","\n","        # Tính Accuracy\n","\n","        # 3.7. Tính độ chính xác trên tập train\n","        acc = cal_acc(parameters, X_train, Y_train)\n","\n","        # 3.8. Tính độ chính xác trên tập validation\n","        val_acc = cal_acc(parameters, X_val, Y_val)\n","\n","        # 3.9. Thực hiện backward trên lớp số 2 để tính ra dA1, dW2, db2\n","        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, 'relu')\n","\n","        # 3.10. Thực hiện backward trên lớp số 1 để tính ra dA0, dW1, db1\n","        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, 'relu')\n","\n","        # 3.11. Cập nhật các giá trị dW1, db1, dW2, db2, dW3, db3 vào dictionary grads\n","        grads['dW1'] = dW1\n","        grads['db1'] = db1\n","        grads['dW2'] = dW2\n","        grads['db2'] = db2\n","        grads['dW3'] = dW3\n","        grads['db3'] = db3\n","\n","        # 3.12. Tiến hành cập nhật các tham số của mô hình\n","        parameters = update_parameters(parameters, grads, learning_rate)\n","\n","        # 3.13. Gán các giá trị tham số mới vào các biến đã được định nghĩa sẵn\n","        W1 = parameters[\"W1\"]\n","        b1 = parameters[\"b1\"]\n","        W2 = parameters[\"W2\"]\n","        b2 = parameters[\"b2\"]\n","        W3 = parameters[\"W3\"]\n","        b3 = parameters[\"b3\"]\n","\n","        print(\"Epoch {}: Cost: {:.3f} Acc: {:.3f} Validation Acc: {:.3f}\".format(i + 1, np.squeeze(cost), acc, val_acc))\n","        costs.append(cost)\n","\n","\n","    return parameters, acc, val_acc, costs"]},{"cell_type":"markdown","metadata":{"id":"T_Vg6Q33R3d2"},"source":["Test code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1698999883285,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"J7i-hWNQkZmu","outputId":"cf4e5e2a-088f-4ea7-c2ff-a08271f5e9f0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((60000, 784), (60000,), (10000, 784), (10000,))"]},"metadata":{},"execution_count":31}],"source":["X_train.shape, Y_train.shape, X_val.shape, Y_val.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13995,"status":"ok","timestamp":1698999897278,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"7UwWV5ltR2wk","outputId":"bea08e88-a609-4387-dac5-fa02dff7229f"},"outputs":[{"output_type":"stream","name":"stdout","text":["(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 1: Cost: 2.303 Acc: 0.086 Validation Acc: 0.081\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 2: Cost: 2.302 Acc: 0.092 Validation Acc: 0.085\n","W1 được cập nhật chưa đúng.\n","b1 được cập nhật chưa đúng.\n","W2 được cập nhật chưa đúng.\n","b2 được cập nhật chưa đúng.\n","W3 được cập nhật đúng.\n","b3 được cập nhật đúng.\n"]}],"source":["try:\n","  parameters, acc, val_acc, costs  = train_model(X_train, Y_train, X_val, Y_val, layers_dims = layers_dims, epochs=2)\n","  exp = {'W1': -3.096, 'W2': -10.642, 'W3': -0.124, 'b1': 0.002, 'b2': 0.007, 'b3': 0.0 }\n","  for param in parameters:\n","    if exp[param] == np.round(np.sum(parameters[param]), 3):\n","      print(\"{} được cập nhật đúng.\".format(param))\n","    else:\n","      print(\"{} được cập nhật chưa đúng.\".format(param))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"a3COa7ZcSJ_5"},"source":["**Kết quả mong đợi**:\n","\n","```\n","Epoch 1: Cost: 2.303 Acc: 0.086 Validation Acc: 0.081\n","Epoch 2: Cost: 2.302 Acc: 0.092 Validation Acc: 0.085\n","W1 được cập nhật đúng.\n","b1 được cập nhật đúng.\n","W2 được cập nhật đúng.\n","b2 được cập nhật đúng.\n","W3 được cập nhật đúng.\n","b3 được cập nhật đúng.\n","```"]},{"cell_type":"markdown","metadata":{"id":"ACVkTBjjTkLH"},"source":["### 7.5. Tiến hành training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HE_sYT2bSYFW","executionInfo":{"status":"ok","timestamp":1699002578439,"user_tz":-420,"elapsed":2681165,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"}},"outputId":"bef69f77-82d7-48b2-ccf0-ac4b0dae7caa"},"outputs":[{"output_type":"stream","name":"stdout","text":["(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 1: Cost: 2.303 Acc: 0.086 Validation Acc: 0.081\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 2: Cost: 2.302 Acc: 0.092 Validation Acc: 0.085\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 3: Cost: 2.302 Acc: 0.123 Validation Acc: 0.117\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 4: Cost: 2.302 Acc: 0.178 Validation Acc: 0.174\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 5: Cost: 2.302 Acc: 0.191 Validation Acc: 0.190\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 6: Cost: 2.302 Acc: 0.189 Validation Acc: 0.187\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 7: Cost: 2.301 Acc: 0.180 Validation Acc: 0.177\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 8: Cost: 2.301 Acc: 0.170 Validation Acc: 0.168\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 9: Cost: 2.301 Acc: 0.161 Validation Acc: 0.160\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 10: Cost: 2.301 Acc: 0.153 Validation Acc: 0.153\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 11: Cost: 2.301 Acc: 0.148 Validation Acc: 0.148\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 12: Cost: 2.301 Acc: 0.143 Validation Acc: 0.144\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 13: Cost: 2.300 Acc: 0.140 Validation Acc: 0.141\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 14: Cost: 2.300 Acc: 0.137 Validation Acc: 0.140\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 15: Cost: 2.300 Acc: 0.135 Validation Acc: 0.138\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 16: Cost: 2.300 Acc: 0.134 Validation Acc: 0.137\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 17: Cost: 2.300 Acc: 0.133 Validation Acc: 0.136\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 18: Cost: 2.299 Acc: 0.132 Validation Acc: 0.136\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 19: Cost: 2.299 Acc: 0.131 Validation Acc: 0.135\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 20: Cost: 2.299 Acc: 0.131 Validation Acc: 0.135\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 21: Cost: 2.299 Acc: 0.131 Validation Acc: 0.135\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 22: Cost: 2.299 Acc: 0.131 Validation Acc: 0.135\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 23: Cost: 2.298 Acc: 0.132 Validation Acc: 0.136\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 24: Cost: 2.298 Acc: 0.132 Validation Acc: 0.136\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 25: Cost: 2.298 Acc: 0.133 Validation Acc: 0.136\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 26: Cost: 2.298 Acc: 0.134 Validation Acc: 0.137\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 27: Cost: 2.298 Acc: 0.135 Validation Acc: 0.138\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 28: Cost: 2.297 Acc: 0.136 Validation Acc: 0.139\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 29: Cost: 2.297 Acc: 0.138 Validation Acc: 0.140\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 30: Cost: 2.297 Acc: 0.139 Validation Acc: 0.141\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 31: Cost: 2.297 Acc: 0.141 Validation Acc: 0.143\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 32: Cost: 2.296 Acc: 0.143 Validation Acc: 0.145\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 33: Cost: 2.296 Acc: 0.145 Validation Acc: 0.147\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 34: Cost: 2.296 Acc: 0.147 Validation Acc: 0.149\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 35: Cost: 2.296 Acc: 0.149 Validation Acc: 0.150\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 36: Cost: 2.295 Acc: 0.151 Validation Acc: 0.153\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 37: Cost: 2.295 Acc: 0.154 Validation Acc: 0.155\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 38: Cost: 2.295 Acc: 0.157 Validation Acc: 0.157\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 39: Cost: 2.294 Acc: 0.159 Validation Acc: 0.158\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 40: Cost: 2.294 Acc: 0.162 Validation Acc: 0.162\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 41: Cost: 2.294 Acc: 0.165 Validation Acc: 0.164\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 42: Cost: 2.293 Acc: 0.168 Validation Acc: 0.167\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 43: Cost: 2.293 Acc: 0.172 Validation Acc: 0.170\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 44: Cost: 2.293 Acc: 0.175 Validation Acc: 0.175\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 45: Cost: 2.292 Acc: 0.179 Validation Acc: 0.179\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 46: Cost: 2.292 Acc: 0.183 Validation Acc: 0.183\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 47: Cost: 2.292 Acc: 0.187 Validation Acc: 0.188\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 48: Cost: 2.291 Acc: 0.192 Validation Acc: 0.194\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 49: Cost: 2.291 Acc: 0.196 Validation Acc: 0.199\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 50: Cost: 2.290 Acc: 0.201 Validation Acc: 0.204\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 51: Cost: 2.290 Acc: 0.207 Validation Acc: 0.210\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 52: Cost: 2.290 Acc: 0.213 Validation Acc: 0.217\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 53: Cost: 2.289 Acc: 0.219 Validation Acc: 0.222\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 54: Cost: 2.289 Acc: 0.225 Validation Acc: 0.228\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 55: Cost: 2.288 Acc: 0.231 Validation Acc: 0.234\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 56: Cost: 2.288 Acc: 0.237 Validation Acc: 0.241\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 57: Cost: 2.287 Acc: 0.243 Validation Acc: 0.248\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 58: Cost: 2.287 Acc: 0.249 Validation Acc: 0.254\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 59: Cost: 2.286 Acc: 0.256 Validation Acc: 0.261\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 60: Cost: 2.285 Acc: 0.263 Validation Acc: 0.268\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 61: Cost: 2.285 Acc: 0.269 Validation Acc: 0.276\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 62: Cost: 2.284 Acc: 0.276 Validation Acc: 0.282\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 63: Cost: 2.284 Acc: 0.282 Validation Acc: 0.288\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 64: Cost: 2.283 Acc: 0.289 Validation Acc: 0.295\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 65: Cost: 2.282 Acc: 0.296 Validation Acc: 0.302\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 66: Cost: 2.281 Acc: 0.301 Validation Acc: 0.308\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 67: Cost: 2.281 Acc: 0.307 Validation Acc: 0.314\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 68: Cost: 2.280 Acc: 0.313 Validation Acc: 0.319\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 69: Cost: 2.279 Acc: 0.317 Validation Acc: 0.325\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 70: Cost: 2.278 Acc: 0.322 Validation Acc: 0.328\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 71: Cost: 2.277 Acc: 0.327 Validation Acc: 0.331\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 72: Cost: 2.276 Acc: 0.330 Validation Acc: 0.333\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 73: Cost: 2.275 Acc: 0.333 Validation Acc: 0.336\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 74: Cost: 2.274 Acc: 0.336 Validation Acc: 0.338\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 75: Cost: 2.273 Acc: 0.339 Validation Acc: 0.339\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 76: Cost: 2.272 Acc: 0.341 Validation Acc: 0.340\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 77: Cost: 2.271 Acc: 0.342 Validation Acc: 0.340\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 78: Cost: 2.270 Acc: 0.343 Validation Acc: 0.341\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 79: Cost: 2.268 Acc: 0.343 Validation Acc: 0.341\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 80: Cost: 2.267 Acc: 0.342 Validation Acc: 0.341\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 81: Cost: 2.266 Acc: 0.341 Validation Acc: 0.340\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 82: Cost: 2.264 Acc: 0.340 Validation Acc: 0.341\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 83: Cost: 2.263 Acc: 0.339 Validation Acc: 0.341\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 84: Cost: 2.261 Acc: 0.336 Validation Acc: 0.337\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 85: Cost: 2.259 Acc: 0.335 Validation Acc: 0.334\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 86: Cost: 2.258 Acc: 0.333 Validation Acc: 0.332\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 87: Cost: 2.256 Acc: 0.331 Validation Acc: 0.331\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 88: Cost: 2.254 Acc: 0.328 Validation Acc: 0.328\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 89: Cost: 2.252 Acc: 0.325 Validation Acc: 0.326\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 90: Cost: 2.250 Acc: 0.322 Validation Acc: 0.324\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 91: Cost: 2.247 Acc: 0.320 Validation Acc: 0.321\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 92: Cost: 2.245 Acc: 0.317 Validation Acc: 0.319\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 93: Cost: 2.242 Acc: 0.314 Validation Acc: 0.317\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 94: Cost: 2.240 Acc: 0.312 Validation Acc: 0.314\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 95: Cost: 2.237 Acc: 0.309 Validation Acc: 0.311\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 96: Cost: 2.234 Acc: 0.307 Validation Acc: 0.308\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 97: Cost: 2.231 Acc: 0.305 Validation Acc: 0.306\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 98: Cost: 2.227 Acc: 0.303 Validation Acc: 0.302\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 99: Cost: 2.224 Acc: 0.301 Validation Acc: 0.300\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 100: Cost: 2.220 Acc: 0.299 Validation Acc: 0.298\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 101: Cost: 2.217 Acc: 0.297 Validation Acc: 0.297\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 102: Cost: 2.213 Acc: 0.296 Validation Acc: 0.296\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 103: Cost: 2.208 Acc: 0.295 Validation Acc: 0.297\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 104: Cost: 2.204 Acc: 0.294 Validation Acc: 0.296\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 105: Cost: 2.199 Acc: 0.294 Validation Acc: 0.296\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 106: Cost: 2.194 Acc: 0.294 Validation Acc: 0.297\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 107: Cost: 2.189 Acc: 0.295 Validation Acc: 0.298\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 108: Cost: 2.184 Acc: 0.296 Validation Acc: 0.298\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 109: Cost: 2.178 Acc: 0.297 Validation Acc: 0.300\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 110: Cost: 2.172 Acc: 0.299 Validation Acc: 0.301\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 111: Cost: 2.166 Acc: 0.302 Validation Acc: 0.304\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 112: Cost: 2.159 Acc: 0.305 Validation Acc: 0.308\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 113: Cost: 2.152 Acc: 0.308 Validation Acc: 0.310\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 114: Cost: 2.145 Acc: 0.313 Validation Acc: 0.315\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 115: Cost: 2.137 Acc: 0.318 Validation Acc: 0.320\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 116: Cost: 2.130 Acc: 0.323 Validation Acc: 0.327\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 117: Cost: 2.121 Acc: 0.329 Validation Acc: 0.334\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 118: Cost: 2.113 Acc: 0.337 Validation Acc: 0.342\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 119: Cost: 2.104 Acc: 0.345 Validation Acc: 0.349\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 120: Cost: 2.095 Acc: 0.353 Validation Acc: 0.358\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 121: Cost: 2.085 Acc: 0.362 Validation Acc: 0.368\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 122: Cost: 2.075 Acc: 0.371 Validation Acc: 0.378\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 123: Cost: 2.065 Acc: 0.381 Validation Acc: 0.388\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 124: Cost: 2.054 Acc: 0.391 Validation Acc: 0.400\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 125: Cost: 2.043 Acc: 0.403 Validation Acc: 0.411\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 126: Cost: 2.031 Acc: 0.415 Validation Acc: 0.422\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 127: Cost: 2.019 Acc: 0.426 Validation Acc: 0.434\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 128: Cost: 2.006 Acc: 0.438 Validation Acc: 0.446\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 129: Cost: 1.993 Acc: 0.450 Validation Acc: 0.457\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 130: Cost: 1.980 Acc: 0.462 Validation Acc: 0.469\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 131: Cost: 1.966 Acc: 0.473 Validation Acc: 0.481\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 132: Cost: 1.951 Acc: 0.485 Validation Acc: 0.493\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 133: Cost: 1.935 Acc: 0.497 Validation Acc: 0.505\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 134: Cost: 1.919 Acc: 0.509 Validation Acc: 0.516\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 135: Cost: 1.903 Acc: 0.520 Validation Acc: 0.527\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 136: Cost: 1.885 Acc: 0.531 Validation Acc: 0.536\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 137: Cost: 1.867 Acc: 0.540 Validation Acc: 0.547\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 138: Cost: 1.849 Acc: 0.550 Validation Acc: 0.558\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 139: Cost: 1.829 Acc: 0.560 Validation Acc: 0.566\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 140: Cost: 1.809 Acc: 0.569 Validation Acc: 0.574\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 141: Cost: 1.788 Acc: 0.577 Validation Acc: 0.582\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 142: Cost: 1.767 Acc: 0.585 Validation Acc: 0.589\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 143: Cost: 1.745 Acc: 0.592 Validation Acc: 0.597\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 144: Cost: 1.722 Acc: 0.598 Validation Acc: 0.605\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 145: Cost: 1.699 Acc: 0.604 Validation Acc: 0.611\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 146: Cost: 1.676 Acc: 0.609 Validation Acc: 0.616\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 147: Cost: 1.652 Acc: 0.614 Validation Acc: 0.621\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 148: Cost: 1.627 Acc: 0.618 Validation Acc: 0.625\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 149: Cost: 1.603 Acc: 0.622 Validation Acc: 0.629\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 150: Cost: 1.578 Acc: 0.625 Validation Acc: 0.631\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 151: Cost: 1.554 Acc: 0.629 Validation Acc: 0.634\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 152: Cost: 1.529 Acc: 0.632 Validation Acc: 0.638\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 153: Cost: 1.504 Acc: 0.635 Validation Acc: 0.641\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 154: Cost: 1.480 Acc: 0.638 Validation Acc: 0.643\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 155: Cost: 1.456 Acc: 0.640 Validation Acc: 0.646\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 156: Cost: 1.432 Acc: 0.643 Validation Acc: 0.648\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 157: Cost: 1.408 Acc: 0.646 Validation Acc: 0.652\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 158: Cost: 1.385 Acc: 0.648 Validation Acc: 0.654\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 159: Cost: 1.362 Acc: 0.651 Validation Acc: 0.658\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 160: Cost: 1.340 Acc: 0.655 Validation Acc: 0.661\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 161: Cost: 1.319 Acc: 0.658 Validation Acc: 0.662\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 162: Cost: 1.297 Acc: 0.662 Validation Acc: 0.667\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 163: Cost: 1.277 Acc: 0.665 Validation Acc: 0.671\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 164: Cost: 1.256 Acc: 0.668 Validation Acc: 0.674\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 165: Cost: 1.237 Acc: 0.671 Validation Acc: 0.676\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 166: Cost: 1.218 Acc: 0.675 Validation Acc: 0.680\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 167: Cost: 1.199 Acc: 0.679 Validation Acc: 0.684\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 168: Cost: 1.181 Acc: 0.682 Validation Acc: 0.689\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 169: Cost: 1.164 Acc: 0.685 Validation Acc: 0.693\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 170: Cost: 1.147 Acc: 0.689 Validation Acc: 0.698\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 171: Cost: 1.131 Acc: 0.692 Validation Acc: 0.700\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 172: Cost: 1.115 Acc: 0.696 Validation Acc: 0.703\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 173: Cost: 1.100 Acc: 0.699 Validation Acc: 0.706\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 174: Cost: 1.085 Acc: 0.702 Validation Acc: 0.710\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 175: Cost: 1.071 Acc: 0.704 Validation Acc: 0.713\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 176: Cost: 1.057 Acc: 0.707 Validation Acc: 0.716\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 177: Cost: 1.044 Acc: 0.709 Validation Acc: 0.718\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 178: Cost: 1.031 Acc: 0.712 Validation Acc: 0.720\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 179: Cost: 1.018 Acc: 0.714 Validation Acc: 0.723\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 180: Cost: 1.006 Acc: 0.717 Validation Acc: 0.725\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 181: Cost: 0.995 Acc: 0.719 Validation Acc: 0.727\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 182: Cost: 0.984 Acc: 0.722 Validation Acc: 0.730\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 183: Cost: 0.973 Acc: 0.724 Validation Acc: 0.731\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 184: Cost: 0.962 Acc: 0.726 Validation Acc: 0.734\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 185: Cost: 0.952 Acc: 0.728 Validation Acc: 0.736\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 186: Cost: 0.942 Acc: 0.730 Validation Acc: 0.738\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 187: Cost: 0.933 Acc: 0.732 Validation Acc: 0.740\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 188: Cost: 0.923 Acc: 0.734 Validation Acc: 0.742\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 189: Cost: 0.914 Acc: 0.736 Validation Acc: 0.744\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 190: Cost: 0.906 Acc: 0.738 Validation Acc: 0.745\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 191: Cost: 0.897 Acc: 0.740 Validation Acc: 0.747\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 192: Cost: 0.889 Acc: 0.741 Validation Acc: 0.749\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 193: Cost: 0.881 Acc: 0.743 Validation Acc: 0.750\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 194: Cost: 0.873 Acc: 0.745 Validation Acc: 0.751\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 195: Cost: 0.866 Acc: 0.747 Validation Acc: 0.754\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 196: Cost: 0.859 Acc: 0.749 Validation Acc: 0.756\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 197: Cost: 0.852 Acc: 0.751 Validation Acc: 0.757\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 198: Cost: 0.845 Acc: 0.752 Validation Acc: 0.759\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 199: Cost: 0.838 Acc: 0.754 Validation Acc: 0.760\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 200: Cost: 0.831 Acc: 0.755 Validation Acc: 0.762\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 201: Cost: 0.825 Acc: 0.757 Validation Acc: 0.764\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 202: Cost: 0.819 Acc: 0.758 Validation Acc: 0.766\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 203: Cost: 0.813 Acc: 0.760 Validation Acc: 0.767\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 204: Cost: 0.807 Acc: 0.762 Validation Acc: 0.768\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 205: Cost: 0.801 Acc: 0.763 Validation Acc: 0.770\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 206: Cost: 0.795 Acc: 0.765 Validation Acc: 0.772\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 207: Cost: 0.790 Acc: 0.766 Validation Acc: 0.773\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 208: Cost: 0.785 Acc: 0.768 Validation Acc: 0.776\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 209: Cost: 0.779 Acc: 0.769 Validation Acc: 0.777\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 210: Cost: 0.774 Acc: 0.771 Validation Acc: 0.778\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 211: Cost: 0.769 Acc: 0.772 Validation Acc: 0.779\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 212: Cost: 0.764 Acc: 0.774 Validation Acc: 0.781\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 213: Cost: 0.759 Acc: 0.776 Validation Acc: 0.782\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 214: Cost: 0.755 Acc: 0.777 Validation Acc: 0.784\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 215: Cost: 0.750 Acc: 0.778 Validation Acc: 0.785\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 216: Cost: 0.745 Acc: 0.780 Validation Acc: 0.787\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 217: Cost: 0.741 Acc: 0.781 Validation Acc: 0.788\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 218: Cost: 0.736 Acc: 0.782 Validation Acc: 0.789\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 219: Cost: 0.732 Acc: 0.784 Validation Acc: 0.791\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 220: Cost: 0.728 Acc: 0.785 Validation Acc: 0.793\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 221: Cost: 0.724 Acc: 0.787 Validation Acc: 0.793\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 222: Cost: 0.720 Acc: 0.788 Validation Acc: 0.795\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 223: Cost: 0.715 Acc: 0.789 Validation Acc: 0.796\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 224: Cost: 0.712 Acc: 0.790 Validation Acc: 0.797\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 225: Cost: 0.708 Acc: 0.791 Validation Acc: 0.798\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 226: Cost: 0.704 Acc: 0.793 Validation Acc: 0.799\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 227: Cost: 0.700 Acc: 0.794 Validation Acc: 0.800\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 228: Cost: 0.696 Acc: 0.795 Validation Acc: 0.802\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 229: Cost: 0.692 Acc: 0.796 Validation Acc: 0.803\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 230: Cost: 0.689 Acc: 0.797 Validation Acc: 0.804\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 231: Cost: 0.685 Acc: 0.798 Validation Acc: 0.805\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 232: Cost: 0.682 Acc: 0.799 Validation Acc: 0.806\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 233: Cost: 0.678 Acc: 0.801 Validation Acc: 0.807\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 234: Cost: 0.675 Acc: 0.801 Validation Acc: 0.809\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 235: Cost: 0.671 Acc: 0.803 Validation Acc: 0.810\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 236: Cost: 0.668 Acc: 0.804 Validation Acc: 0.810\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 237: Cost: 0.665 Acc: 0.805 Validation Acc: 0.811\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 238: Cost: 0.661 Acc: 0.806 Validation Acc: 0.813\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 239: Cost: 0.658 Acc: 0.807 Validation Acc: 0.814\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 240: Cost: 0.655 Acc: 0.808 Validation Acc: 0.814\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 241: Cost: 0.652 Acc: 0.808 Validation Acc: 0.815\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 242: Cost: 0.649 Acc: 0.810 Validation Acc: 0.816\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 243: Cost: 0.646 Acc: 0.811 Validation Acc: 0.817\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 244: Cost: 0.642 Acc: 0.812 Validation Acc: 0.818\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 245: Cost: 0.639 Acc: 0.812 Validation Acc: 0.819\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 246: Cost: 0.636 Acc: 0.814 Validation Acc: 0.819\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 247: Cost: 0.634 Acc: 0.814 Validation Acc: 0.820\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 248: Cost: 0.631 Acc: 0.816 Validation Acc: 0.821\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 249: Cost: 0.628 Acc: 0.816 Validation Acc: 0.822\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 250: Cost: 0.625 Acc: 0.817 Validation Acc: 0.823\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 251: Cost: 0.622 Acc: 0.818 Validation Acc: 0.824\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 252: Cost: 0.619 Acc: 0.819 Validation Acc: 0.825\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 253: Cost: 0.616 Acc: 0.820 Validation Acc: 0.825\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 254: Cost: 0.614 Acc: 0.821 Validation Acc: 0.826\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 255: Cost: 0.611 Acc: 0.822 Validation Acc: 0.826\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 256: Cost: 0.608 Acc: 0.823 Validation Acc: 0.827\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 257: Cost: 0.606 Acc: 0.823 Validation Acc: 0.828\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 258: Cost: 0.603 Acc: 0.824 Validation Acc: 0.829\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 259: Cost: 0.601 Acc: 0.825 Validation Acc: 0.830\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 260: Cost: 0.598 Acc: 0.825 Validation Acc: 0.830\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 261: Cost: 0.595 Acc: 0.826 Validation Acc: 0.832\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 262: Cost: 0.593 Acc: 0.827 Validation Acc: 0.832\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 263: Cost: 0.590 Acc: 0.828 Validation Acc: 0.832\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 264: Cost: 0.588 Acc: 0.829 Validation Acc: 0.833\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 265: Cost: 0.586 Acc: 0.830 Validation Acc: 0.834\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 266: Cost: 0.583 Acc: 0.831 Validation Acc: 0.835\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 267: Cost: 0.581 Acc: 0.831 Validation Acc: 0.836\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 268: Cost: 0.578 Acc: 0.832 Validation Acc: 0.837\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 269: Cost: 0.576 Acc: 0.833 Validation Acc: 0.838\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 270: Cost: 0.574 Acc: 0.833 Validation Acc: 0.839\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 271: Cost: 0.572 Acc: 0.834 Validation Acc: 0.840\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 272: Cost: 0.569 Acc: 0.835 Validation Acc: 0.841\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 273: Cost: 0.567 Acc: 0.836 Validation Acc: 0.842\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 274: Cost: 0.565 Acc: 0.836 Validation Acc: 0.842\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 275: Cost: 0.563 Acc: 0.837 Validation Acc: 0.843\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 276: Cost: 0.561 Acc: 0.838 Validation Acc: 0.844\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 277: Cost: 0.559 Acc: 0.838 Validation Acc: 0.844\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 278: Cost: 0.556 Acc: 0.840 Validation Acc: 0.845\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 279: Cost: 0.554 Acc: 0.840 Validation Acc: 0.845\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 280: Cost: 0.552 Acc: 0.841 Validation Acc: 0.845\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 281: Cost: 0.550 Acc: 0.842 Validation Acc: 0.846\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 282: Cost: 0.548 Acc: 0.842 Validation Acc: 0.846\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 283: Cost: 0.546 Acc: 0.843 Validation Acc: 0.847\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 284: Cost: 0.544 Acc: 0.844 Validation Acc: 0.848\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 285: Cost: 0.542 Acc: 0.844 Validation Acc: 0.848\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 286: Cost: 0.540 Acc: 0.845 Validation Acc: 0.849\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 287: Cost: 0.539 Acc: 0.846 Validation Acc: 0.850\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 288: Cost: 0.537 Acc: 0.846 Validation Acc: 0.850\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 289: Cost: 0.535 Acc: 0.847 Validation Acc: 0.850\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 290: Cost: 0.533 Acc: 0.847 Validation Acc: 0.851\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 291: Cost: 0.531 Acc: 0.848 Validation Acc: 0.852\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 292: Cost: 0.529 Acc: 0.848 Validation Acc: 0.852\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 293: Cost: 0.528 Acc: 0.849 Validation Acc: 0.853\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 294: Cost: 0.526 Acc: 0.850 Validation Acc: 0.853\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 295: Cost: 0.524 Acc: 0.850 Validation Acc: 0.853\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 296: Cost: 0.522 Acc: 0.851 Validation Acc: 0.853\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 297: Cost: 0.521 Acc: 0.851 Validation Acc: 0.854\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 298: Cost: 0.519 Acc: 0.852 Validation Acc: 0.855\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 299: Cost: 0.517 Acc: 0.852 Validation Acc: 0.855\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 300: Cost: 0.516 Acc: 0.852 Validation Acc: 0.855\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 301: Cost: 0.514 Acc: 0.853 Validation Acc: 0.856\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 302: Cost: 0.513 Acc: 0.853 Validation Acc: 0.857\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 303: Cost: 0.511 Acc: 0.854 Validation Acc: 0.857\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 304: Cost: 0.509 Acc: 0.854 Validation Acc: 0.858\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 305: Cost: 0.508 Acc: 0.855 Validation Acc: 0.858\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 306: Cost: 0.506 Acc: 0.855 Validation Acc: 0.859\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 307: Cost: 0.505 Acc: 0.856 Validation Acc: 0.859\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 308: Cost: 0.503 Acc: 0.856 Validation Acc: 0.860\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 309: Cost: 0.502 Acc: 0.857 Validation Acc: 0.861\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 310: Cost: 0.500 Acc: 0.857 Validation Acc: 0.861\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 311: Cost: 0.499 Acc: 0.857 Validation Acc: 0.861\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 312: Cost: 0.497 Acc: 0.858 Validation Acc: 0.862\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 313: Cost: 0.496 Acc: 0.858 Validation Acc: 0.862\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 314: Cost: 0.495 Acc: 0.859 Validation Acc: 0.863\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 315: Cost: 0.493 Acc: 0.859 Validation Acc: 0.863\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 316: Cost: 0.492 Acc: 0.859 Validation Acc: 0.863\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 317: Cost: 0.490 Acc: 0.860 Validation Acc: 0.864\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 318: Cost: 0.489 Acc: 0.860 Validation Acc: 0.864\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 319: Cost: 0.488 Acc: 0.861 Validation Acc: 0.864\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 320: Cost: 0.486 Acc: 0.861 Validation Acc: 0.864\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 321: Cost: 0.485 Acc: 0.861 Validation Acc: 0.865\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 322: Cost: 0.484 Acc: 0.862 Validation Acc: 0.865\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 323: Cost: 0.482 Acc: 0.862 Validation Acc: 0.866\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 324: Cost: 0.481 Acc: 0.862 Validation Acc: 0.866\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 325: Cost: 0.480 Acc: 0.863 Validation Acc: 0.866\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 326: Cost: 0.479 Acc: 0.863 Validation Acc: 0.867\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 327: Cost: 0.477 Acc: 0.863 Validation Acc: 0.867\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 328: Cost: 0.476 Acc: 0.864 Validation Acc: 0.867\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 329: Cost: 0.475 Acc: 0.864 Validation Acc: 0.867\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 330: Cost: 0.474 Acc: 0.865 Validation Acc: 0.868\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 331: Cost: 0.473 Acc: 0.865 Validation Acc: 0.868\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 332: Cost: 0.471 Acc: 0.866 Validation Acc: 0.868\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 333: Cost: 0.470 Acc: 0.866 Validation Acc: 0.869\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 334: Cost: 0.469 Acc: 0.866 Validation Acc: 0.869\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 335: Cost: 0.468 Acc: 0.867 Validation Acc: 0.869\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 336: Cost: 0.467 Acc: 0.867 Validation Acc: 0.870\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 337: Cost: 0.466 Acc: 0.867 Validation Acc: 0.870\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 338: Cost: 0.465 Acc: 0.868 Validation Acc: 0.870\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 339: Cost: 0.464 Acc: 0.868 Validation Acc: 0.871\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 340: Cost: 0.462 Acc: 0.868 Validation Acc: 0.871\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 341: Cost: 0.461 Acc: 0.868 Validation Acc: 0.871\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 342: Cost: 0.460 Acc: 0.869 Validation Acc: 0.871\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 343: Cost: 0.459 Acc: 0.869 Validation Acc: 0.872\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 344: Cost: 0.458 Acc: 0.869 Validation Acc: 0.872\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 345: Cost: 0.457 Acc: 0.869 Validation Acc: 0.872\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 346: Cost: 0.456 Acc: 0.870 Validation Acc: 0.873\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 347: Cost: 0.455 Acc: 0.870 Validation Acc: 0.873\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 348: Cost: 0.454 Acc: 0.870 Validation Acc: 0.874\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 349: Cost: 0.453 Acc: 0.871 Validation Acc: 0.874\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 350: Cost: 0.452 Acc: 0.871 Validation Acc: 0.874\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 351: Cost: 0.451 Acc: 0.871 Validation Acc: 0.874\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 352: Cost: 0.450 Acc: 0.872 Validation Acc: 0.875\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 353: Cost: 0.449 Acc: 0.872 Validation Acc: 0.875\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 354: Cost: 0.448 Acc: 0.872 Validation Acc: 0.875\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 355: Cost: 0.447 Acc: 0.872 Validation Acc: 0.875\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 356: Cost: 0.446 Acc: 0.873 Validation Acc: 0.875\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 357: Cost: 0.445 Acc: 0.873 Validation Acc: 0.875\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 358: Cost: 0.444 Acc: 0.873 Validation Acc: 0.876\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 359: Cost: 0.444 Acc: 0.873 Validation Acc: 0.875\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 360: Cost: 0.443 Acc: 0.874 Validation Acc: 0.876\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 361: Cost: 0.442 Acc: 0.874 Validation Acc: 0.877\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 362: Cost: 0.441 Acc: 0.874 Validation Acc: 0.876\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 363: Cost: 0.440 Acc: 0.874 Validation Acc: 0.877\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 364: Cost: 0.439 Acc: 0.875 Validation Acc: 0.877\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 365: Cost: 0.438 Acc: 0.875 Validation Acc: 0.877\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 366: Cost: 0.437 Acc: 0.875 Validation Acc: 0.877\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 367: Cost: 0.437 Acc: 0.875 Validation Acc: 0.878\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 368: Cost: 0.436 Acc: 0.876 Validation Acc: 0.878\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 369: Cost: 0.435 Acc: 0.876 Validation Acc: 0.879\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 370: Cost: 0.434 Acc: 0.876 Validation Acc: 0.879\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 371: Cost: 0.433 Acc: 0.877 Validation Acc: 0.879\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 372: Cost: 0.432 Acc: 0.877 Validation Acc: 0.880\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 373: Cost: 0.432 Acc: 0.877 Validation Acc: 0.880\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 374: Cost: 0.431 Acc: 0.877 Validation Acc: 0.880\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 375: Cost: 0.430 Acc: 0.878 Validation Acc: 0.880\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 376: Cost: 0.429 Acc: 0.878 Validation Acc: 0.880\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 377: Cost: 0.429 Acc: 0.878 Validation Acc: 0.880\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 378: Cost: 0.428 Acc: 0.878 Validation Acc: 0.881\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 379: Cost: 0.427 Acc: 0.879 Validation Acc: 0.881\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 380: Cost: 0.426 Acc: 0.879 Validation Acc: 0.881\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 381: Cost: 0.425 Acc: 0.879 Validation Acc: 0.881\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 382: Cost: 0.425 Acc: 0.879 Validation Acc: 0.881\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 383: Cost: 0.424 Acc: 0.880 Validation Acc: 0.881\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 384: Cost: 0.423 Acc: 0.880 Validation Acc: 0.882\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 385: Cost: 0.423 Acc: 0.880 Validation Acc: 0.882\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 386: Cost: 0.422 Acc: 0.880 Validation Acc: 0.882\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 387: Cost: 0.421 Acc: 0.880 Validation Acc: 0.882\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 388: Cost: 0.420 Acc: 0.881 Validation Acc: 0.883\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 389: Cost: 0.420 Acc: 0.881 Validation Acc: 0.883\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 390: Cost: 0.419 Acc: 0.881 Validation Acc: 0.883\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 391: Cost: 0.418 Acc: 0.881 Validation Acc: 0.884\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 392: Cost: 0.418 Acc: 0.881 Validation Acc: 0.884\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 393: Cost: 0.417 Acc: 0.882 Validation Acc: 0.884\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 394: Cost: 0.416 Acc: 0.882 Validation Acc: 0.884\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 395: Cost: 0.416 Acc: 0.882 Validation Acc: 0.884\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 396: Cost: 0.415 Acc: 0.882 Validation Acc: 0.884\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 397: Cost: 0.414 Acc: 0.882 Validation Acc: 0.884\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 398: Cost: 0.414 Acc: 0.882 Validation Acc: 0.884\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 399: Cost: 0.413 Acc: 0.883 Validation Acc: 0.884\n","(10, 60000) (60000,)\n","(10, 60000) (60000,)\n","(10, 10000) (10000,)\n","Epoch 400: Cost: 0.412 Acc: 0.883 Validation Acc: 0.885\n"]}],"source":["try:\n","  # plot the cost\n","  epochs = 400\n","  learning_rate = 0.1\n","  parameters, acc, val_acc, costs  = train_model(X_train, Y_train, X_val, Y_val, layers_dims = layers_dims, learning_rate=learning_rate, epochs=epochs)\n","\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"0KNolg09TuSI"},"source":["**Kết quả mong đợi (Tương đối)**:\n","\n","```\n","Epoch 130: Cost: 0.416 Acc: 0.877 Validation Acc: 0.884\n","Epoch 131: Cost: 0.412 Acc: 0.883 Validation Acc: 0.886\n","Epoch 132: Cost: 0.410 Acc: 0.879 Validation Acc: 0.885\n","Epoch 133: Cost: 0.407 Acc: 0.885 Validation Acc: 0.887\n","Epoch 134: Cost: 0.406 Acc: 0.880 Validation Acc: 0.887\n","Epoch 135: Cost: 0.403 Acc: 0.886 Validation Acc: 0.888\n","Epoch 136: Cost: 0.403 Acc: 0.881 Validation Acc: 0.889\n","Epoch 137: Cost: 0.400 Acc: 0.886 Validation Acc: 0.888\n","Epoch 138: Cost: 0.401 Acc: 0.882 Validation Acc: 0.890\n","Epoch 139: Cost: 0.398 Acc: 0.887 Validation Acc: 0.888\n","Epoch 140: Cost: 0.400 Acc: 0.882 Validation Acc: 0.890\n","Epoch 141: Cost: 0.397 Acc: 0.887 Validation Acc: 0.888\n","Epoch 142: Cost: 0.399 Acc: 0.882 Validation Acc: 0.889\n","Epoch 143: Cost: 0.396 Acc: 0.887 Validation Acc: 0.888\n","Epoch 144: Cost: 0.399 Acc: 0.881 Validation Acc: 0.890\n","Epoch 145: Cost: 0.395 Acc: 0.887 Validation Acc: 0.888\n","Epoch 146: Cost: 0.398 Acc: 0.881 Validation Acc: 0.890\n","Epoch 147: Cost: 0.394 Acc: 0.886 Validation Acc: 0.888\n","Epoch 148: Cost: 0.396 Acc: 0.881 Validation Acc: 0.890\n","Epoch 149: Cost: 0.392 Acc: 0.886 Validation Acc: 0.888\n","Epoch 150: Cost: 0.394 Acc: 0.882 Validation Acc: 0.891\n","Epoch 151: Cost: 0.389 Acc: 0.887 Validation Acc: 0.889\n","Epoch 152: Cost: 0.389 Acc: 0.884 Validation Acc: 0.892\n","Epoch 153: Cost: 0.384 Acc: 0.889 Validation Acc: 0.890\n","Epoch 154: Cost: 0.383 Acc: 0.887 Validation Acc: 0.893\n","Epoch 155: Cost: 0.378 Acc: 0.891 Validation Acc: 0.892\n","Epoch 156: Cost: 0.376 Acc: 0.890 Validation Acc: 0.896\n","Epoch 157: Cost: 0.372 Acc: 0.893 Validation Acc: 0.895\n","Epoch 158: Cost: 0.370 Acc: 0.893 Validation Acc: 0.899\n","Epoch 159: Cost: 0.367 Acc: 0.895 Validation Acc: 0.898\n","Epoch 160: Cost: 0.364 Acc: 0.895 Validation Acc: 0.901\n","Epoch 161: Cost: 0.361 Acc: 0.897 Validation Acc: 0.899\n","Epoch 162: Cost: 0.359 Acc: 0.897 Validation Acc: 0.902\n","Epoch 163: Cost: 0.357 Acc: 0.899 Validation Acc: 0.901\n","Epoch 164: Cost: 0.355 Acc: 0.899 Validation Acc: 0.904\n","Epoch 165: Cost: 0.353 Acc: 0.900 Validation Acc: 0.902\n","Epoch 166: Cost: 0.352 Acc: 0.900 Validation Acc: 0.904\n","Epoch 167: Cost: 0.350 Acc: 0.901 Validation Acc: 0.903\n","Epoch 168: Cost: 0.349 Acc: 0.901 Validation Acc: 0.906\n","Epoch 169: Cost: 0.347 Acc: 0.902 Validation Acc: 0.904\n","Epoch 170: Cost: 0.346 Acc: 0.902 Validation Acc: 0.906\n","```"]},{"cell_type":"markdown","metadata":{"id":"4WyVklEEKNFp"},"source":["Chúc mừng bạn đã vượt qua một thử thách lớn trong con đường học AI của mình."]},{"cell_type":"markdown","metadata":{"id":"8Fux1mJWKeqU"},"source":["<img src=\"https://storage.googleapis.com/protonx-cloud-storage/icons/488-bicycle-outline.gif\" />"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1FpruQCcRFTZ7be4TviWjSpszD6vAgjK2","timestamp":1705726684022},{"file_id":"1dQtbWnc0r1LoKMQLZYNRhwPX6VXCWwll","timestamp":1603544333221},{"file_id":"https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb","timestamp":1597203092580}]},"coursera":{"course_slug":"neural-networks-deep-learning","graded_item_id":"c4HO0","launcher_item_id":"lSYZM"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}