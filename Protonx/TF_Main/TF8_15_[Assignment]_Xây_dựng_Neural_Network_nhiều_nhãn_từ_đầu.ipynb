{"nbformat":4,"nbformat_minor":0,"metadata":{"coursera":{"course_slug":"neural-networks-deep-learning","graded_item_id":"c4HO0","launcher_item_id":"lSYZM"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"provenance":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DNskF-vZx2cy"},"source":["# Bài tập lập trình: Thực hiện xây dựng mạng Neural Network\n","```\n","ProtonX - TensorFlow Class\n","```\n","\n","### Hướng dẫn làm bài\n","- Trong bài tập này bạn sẽ sử dụng Python 3.\n","- Sau khi bạn viết Code của mình xong, hãy chạy dòng Code đó để xem kết quả bên dưới.\n","\n","### [Quan trọng] Chú ý\n","- **Không sử dụng hàm `input()` tại bất kỳ dòng lệnh nào**\n","- **Không thay đổi dòng code return của hàm**\n","\n","Các bạn sẽ thực hiện `code` trong các phần hiển thị `#TODO: Lập trình tại đây` và thay thế các vị trí `None`. Có những câu hỏi chỉ cần trả về đáp án.\n","\n","Sau khi viết xong Code của bạn, bạn hãy ấn \"SHIFT\"+\"ENTER\" để thực hiện chạy lệnh của Cell đó.\n","\n","---\n","Điểm số:\n","* 10 điểm / Câu\n","\n","Tiêu chí chấm điểm:\n","* Các bài tập sẽ được chấm dựa trên các Test-case.\n","* Các bạn không khởi tạo lại giá trị đầu vào bên trong hàm. Có thể khởi tạo các giá trị này ngoài hàm nhằm mục đích kiểm thử."]},{"cell_type":"markdown","metadata":{"id":"IZG7hNuyuPCJ"},"source":["# Thực hiện xây dựng mạng Neural Network\n","\n","Trong tuần này, bạn sẽ thực hiện xây dựng Neural Network đơn giản với:\n","- 2 lớp ẩn với hàm phi tuyến Relu\n","- một lớp phân loại softmax\n","\n","Bạn có thể mở rộng notebook này để có mạng nhiều lớp hơn."]},{"cell_type":"markdown","metadata":{"id":"lZTfsvxWssun"},"source":["## 1 - Khai báo một số hàm bổ sung"]},{"cell_type":"code","metadata":{"id":"MEseTbtQplF8"},"source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"XpT1677QuPCV"},"source":["## 2 - Quá trình thực hiện\n","\n","Quá trình toàn bộ mô hình sẽ theo các bước sau\n","\n","- Khởi tạo các tham số W, b.\n","- Triển khai `Forward Propagation Module` (lan truyền thuận). Như đường màu tím trong hình bên dưới.\n","- Tính giá trị mất mát cost.\n","- Thực hiện `Backward Propagation Module` (lan truyền ngược). Như đường màu xanh trong hình bên dưới.\n","- Cập nhật các tham số.\n","\n","<img src=\"https://storage.googleapis.com/protonx-cloud-storage/images/Feedforward.PNG\" style=\"width:800px;height:500px;\">\n","<caption><center> Ảnh 1</center></caption><br>\n","\n","**Lưu ý**: với mỗi forward function, sẽ có backward function tương ứng. Đó là lý do mà ở mỗi forward module chúng ta sẽ lưu lại các giá trị này trong `cache` để thuận tiện cho việc tính `gradients`.\n"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"KKhUIXrkuPCW"},"source":["## 3 - Khởi tạo\n"]},{"cell_type":"markdown","metadata":{"id":"7rimzl3wumQo"},"source":["**Hướng dẫn**:\n","- Khởi tạo giá trị random cho ma trận. Sử dụng `np.random.randn(shape)*0.01`.\n","- Khởi tạo `0` cho bias. Sử dụng `np.zeros(shape)`."]},{"cell_type":"markdown","metadata":{"id":"Y_x3JL9zkGYu"},"source":["Các bộ tham số giữa các lớp sẽ có chiều như sau:\n","\n","- Lớp 1:\n","  - $\\textbf{W}^{(1)} \\in \\mathbf{R}^{d^{(1)} \\times  n } $\n","  - $b^{(1)} \\in \\mathbf{R}^{ d^{(1)} \\times 1 }$\n","- Lớp 2:\n","  - $\\textbf{W}^{(2)} \\in \\mathbf{R}^{d^{(2)} \\times d^{(1)}} $\n","  - $b^{(2)} \\in \\mathbf{R}^{ d^{(2)} \\times 1 }$\n","- Lớp 3:\n","  - $\\textbf{W}^{(3)} \\in \\mathbf{R}^{c \\times d^{(2)}} $\n","  - $b^{(3)} \\in \\mathbf{R}^{ c \\times 1 }$\n","\n","Biến `num_classes` đại diện cho số nhãn $c$ trong trường hợp này."]},{"cell_type":"code","metadata":{"id":"fSoEHkUJuPCX"},"source":["def initialize_parameters(n, d_1, d_2, num_classes):\n","    \"\"\"\n","    Hàm khởi tạo tham số ban đầu\n","    Đầu vào:\n","      n: int\n","        Chiều của x đầu vào\n","      d_1: int\n","        Chiều của lớp ẩn 1\n","      d_2:\n","        Chiều của lớp ẩn 2\n","      num_classes:\n","        Số lượng nhãn\n","    Đầu ra:\n","      parameters: python dictionary\n","      Bao gồm:\n","        W1: ma trận W1 lớp 1 với chiều (d_1, n)\n","        b1: vector bias b1 lớp 1 với chiều (d_1, 1)\n","        W2: ma trận W2 lớp 2 với chiều (d_2, d_1)\n","        b2: vector bias b2 lớp 2 với chiều (d_2, 1)\n","        W3:  ma trận W3 lớp 3 với chiều (num_classes, d_2)\n","        b3: vector bias b3 lớp 3 với chiều (num_classes, 1)\n","    \"\"\"\n","\n","    # Khởi tạo tham số\n","\n","    np.random.seed(42)\n","    W1 = np.random.randn(d_1, n) * 0.01\n","    b1 = np.zeros((d_1, 1))\n","    W2 = np.random.randn(d_2, d_1) * 0.01\n","    b2 = np.zeros((d_2, 1))\n","    W3 = np.random.randn(num_classes, d_2) * 0.01\n","    b3 = np.zeros((num_classes, 1))\n","\n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2,\n","                  \"W3\": W3,\n","                  \"b3\": b3,\n","                  }\n","\n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"1H9ylZ8vuPCc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"63ea75c8-4987-41fc-f5b4-e59b8c932f78"},"source":["parameters = initialize_parameters(784, 256, 128, 10)\n","print(\"W1.shape = {}\".format(parameters[\"W1\"].shape))\n","print(\"b1.shape = {}\".format(parameters[\"b1\"].shape))\n","print(\"W2.shape = {}\".format(parameters[\"W2\"].shape))\n","print(\"b2.shape = {}\".format(parameters[\"b2\"].shape))\n","print(\"W3.shape = {}\".format(parameters[\"W3\"].shape))\n","print(\"b3.shape = {}\".format(parameters[\"b3\"].shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["W1.shape = (256, 784)\n","b1.shape = (256, 1)\n","W2.shape = (128, 256)\n","b2.shape = (128, 1)\n","W3.shape = (10, 128)\n","b3.shape = (10, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BA0wIS9ouPCs"},"source":["## 4 - Lan truyền thuận (Forward propagation module)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xJ4OlH6Ru3uB"},"source":["### 4.1 - Lan truyền thuận tuyến tính (Linear Forward)\n","\n","Sau khi khởi tạo các tham số cần thiết, bây giờ bạn sẽ thực hiện forward propagation module. Bạn sẽ thực hiện theo thứ tự sau:\n","\n","- Tuyến tính (Linear)\n","- Tuyến tính $\\rightarrow $ Phi tuyến (Linear $\\rightarrow $ Activation)\n","\n","\n","#### 4.1.1. Trên 1 điểm dữ liệu\n","\n","\n","Công thức lan truyền thuận tuyến tính như sau:\n","\n","$$\\textbf{z}^{(l)} = \\textbf{W}^{(l)}\\textbf{a}^{(l-1)} +\\textbf{b}^{(l)}\\tag{4}$$\n","\n","trong đó $\\textbf{a}^{(0)} = \\textbf{x}$.\n","\n","\n","#### 4.1.2. Trên toàn tập dữ liệu\n","\n","\n","Công thức Linear Forward như sau:\n","\n","\n","\n","$$\\textbf{Z}^{(l)} = \\textbf{W}^{(l)}\\textbf{A}^{(l-1)} +\\textbf{b}^{(l)}\\tag{5}$$\n","\n","\n","trong đó $\\textbf{A}^{(0)} = \\textbf{X}$\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"iB3SvCIhuPCt"},"source":["def linear_forward(A_pre, W, b):\n","    \"\"\"\n","    Tiến hành lan truyền thuận tuyến tính (linear forward)\n","    Đầu vào:\n","      A_pre:\n","        Dạng: numpy array\n","        Miêu tả: Đầu ra của lớp phía trước trên toàn bộ tập dữ liệu\n","        Chiều: (Chiều của lớp phía trước, Số lượng điểm dữ liệu)\n","      W:\n","        Dạng: numpy array\n","        Miêu tả: Ma trận tham số của lớp hiện tại\n","        Chiều: (Chiều của lớp hiện tại, Chiều của lớp phía trước)\n","      b:\n","        Dạng: numpy array\n","        Miêu tả: Vector bias của lớp hiện tại\n","        Chiều: (Chiều của lớp hiện tại, 1)\n","    Đầu ra:\n","      Z:\n","        Dạng: numpy array\n","        Miêu tả: Đầu ra của phép nhân tuyến tính và cũng là đầu vào của hàm activation\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","      cache:\n","        Dạng: python tuple\n","        Miêu tả: Bao gồm các biến A_pre, W và b\n","    \"\"\"\n","\n","    Z = np.dot(W, A_pre) + b\n","\n","    cache = (A_pre, W, b)\n","\n","    return Z, cache"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l4CtxMAHUGw0"},"source":["Test code"]},{"cell_type":"code","metadata":{"id":"mPxh6ht6uPCx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"20391212-02e5-4968-db57-235108253bb5"},"source":["try:\n","  A1 = np.ones((256, 60000))\n","  W2 = np.random.normal(size=((128, 256)))\n","  b2 = np.ones(((128, 1)))\n","\n","  Z, linear_cache = linear_forward(A1, W2, b2)\n","  print(\"Z.shape: {}\".format(Z.shape))\n","except Exception as e:\n","  print(\"Lỗi thực thi: {}\", e)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Z.shape: (128, 60000)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LXNBmND6fV-z"},"source":["### 4.2. Lan truyền thuận phi tuyến (Activation Forward)"]},{"cell_type":"markdown","metadata":{"id":"whvJKWdLfoXA"},"source":["#### 4.2.1 Công thức RELU\n","\n","$$f(z) = \\left\\{\\begin{matrix}\n","0 \\ \\forall z \\leq 0 \\\\\n","z \\ \\forall z > 0 \\\\\n","\\end{matrix}\\right. = \\max\\left \\{0, z  \\right \\} = z\\textbf{1}_{z>1}$$"]},{"cell_type":"markdown","metadata":{"id":"TyPiNkq7gNhv"},"source":["**Chú ý:** Các phép tính trong bài toán này đều sử dụng ma trận cho thuật toán Gradient Descent thay vì vector trong bài tập Softmax"]},{"cell_type":"code","metadata":{"id":"G7Csx0CHoeZn"},"source":["def relu(Z):\n","    \"\"\"\n","    Thực hiện hàm RELU trên ma trận Z\n","    Đầu vào:\n","      Z:\n","        Dạng: numpy array\n","        Miêu tả: Ma trận Z. Đầu ra của phép nhân tuyến tính\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","        Ví dụ: (256, 60000)\n","    Đầu ra:\n","      A:\n","        Dạng: numpy array\n","        Miêu tả: Giá trị A sau khi đưa Z qua Relu\n","        Chiều: (Chiều của lớp hiện tại, 60000)\n","        Ví dụ: (256, 60000)\n","      cache:\n","        Dạng: numpy array\n","        Miêu tả:\n","          Ma trận Z. Đầu ra của phép nhân tuyến tính (hay đầu vào của phép phi tuyến)\n","          Việc lưu trữ này để sử dụng cho thuật toán lan truyền ngược\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","        Ví dụ: (256, 60000)\n","    \"\"\"\n","\n","    # Lập trình tại đây\n","\n","    A = np.maximum(0,Z)\n","\n","    cache = Z\n","\n","    return A, cache"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_XONCPgw-Qsf"},"source":["#### 4.2.2. Công thức Softmax\n"]},{"cell_type":"markdown","metadata":{"id":"qK1_psFQgFr8"},"source":["\n","Trong trường hợp này ta sẽ lấy softmax trên từng cột của $\\textbf{Z}$ với $\\textbf{Z} \\in \\mathbf{R}^{c \\times m} $\n","\n","Với:\n","\n","- $c$: Số nhãn\n","- $m$: Số lượng điễm dữ liệu\n","\n","\n","$$\\hat{\\textbf{Y}} = \\text{softmax}(\\textbf{Z}) =  \\begin{bmatrix}\n","\\text{softmax}(\\begin{bmatrix}\n","z_1^1 \\\\\n","\\ \\vdots \\\\\n","z_c^1 \\\\\n","\\end{bmatrix}) & \\text{softmax}(\\begin{bmatrix}\n","z_1^2 \\\\\n","\\ \\vdots \\\\\n","z_c^2 \\\\\n","\\end{bmatrix}) & \\dots & \\text{softmax}(\\begin{bmatrix}\n","z_1^m \\\\\n","\\ \\vdots \\\\\n","z_c^m \\\\\n","\\end{bmatrix})\n","\\end{bmatrix} $$"]},{"cell_type":"code","metadata":{"id":"_wmZVN138GKW"},"source":["def softmax(Z):\n","  \"\"\"\n","  Thực hiện hàm softmax trên ma trận Z\n","  Đầu vào:\n","    Z:\n","      Dạng: numpy array\n","      Miêu tả: Ma trận Z. Đầu ra của phép nhân tuyến tính\n","      Chiều: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","      Ví dụ: (10, 60000)\n","  Đầu ra:\n","    Y_hat:\n","      Dạng: numpy array\n","      Miêu tả: Chuẩn hóa ma trận Z theo phân phối softmax có tổng các giá trị bằng 1\n","      Chiều: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","      Ví dụ: (10, 60000)\n","    cache:\n","      Dạng: numpy array\n","      Miêu tả:\n","        Ma trận Z. Đầu ra của phép nhân tuyến tính\n","        Việc lưu trữ này để sử dụng cho thuật toán lan truyền ngược\n","      Chiều: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","      Ví dụ: (10, 60000)\n","  \"\"\"\n","\n","  e_Z = np.exp(Z)\n","\n","  Y_hat = e_Z/ e_Z.sum(axis=0)\n","\n","  cache = Z\n","\n","  return Y_hat, cache"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j3rPCVP3jGw1"},"source":["Test code\n"]},{"cell_type":"markdown","metadata":{"id":"mNcZEbYY-c3k"},"source":["Kiểm tra lại các cột sau khi sử dụng Softmax có cho kết quả tổng bằng 1 hay không."]},{"cell_type":"code","metadata":{"id":"vQm9e4cv8ItL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"76879ce5-0802-4c3e-deaa-cf4bbf54ac5d"},"source":["Z = np.ones(((10, 60000)))\n","Y_hat, _ = softmax(Z)\n","\n","np.sum(Y_hat[:,3])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"EJMHwVRIuPC3"},"source":["### 4.3 - Lan truyền thuận trên một lớp (Linear-Activation Forward)\n","\n","Trong notebook này, bạn sẽ sử dụng activation function `RELU`:\n","\n","<!-- - **Sigmoid**: $ \\text{Sigmoid}(\\textbf{Z}) = \\sigma(\\textbf{Z}) = \\sigma(\\textbf{W} \\textbf{A} + \\textbf{b}) = \\frac{1}{ 1 + e^{-(\\textbf{W} \\textbf{A} + \\textbf{b})}}$. Function này sẽ trả về 2 giá trị \"`A`\" (giá trị activation) và \"`cache`\" (chứa giá trị của `Z`) - Các giá trị này sẽ được sử dụng cho Backward Function. Để sử dụng, bạn chỉ cần gọi theo mẫu sau:\n","```python\n","A, activation_cache = sigmoid(Z)\n","``` -->\n","\n","- **ReLU**: công thức toán của ReLU là $\\textbf{A} = \\text{Relu}(\\textbf{Z}) = \\text{max}(0, \\textbf{Z})$.  Function này sẽ trả về 2 giá trị \"`a`\" (giá trị activation) và \"`cache`\" (chứa giá trị của `Z`) - Các giá trị này sẽ được sử dụng cho Backward Function. Để sử dụng, bạn chỉ cần gọi theo mẫu sau:\n","``` python\n","A, activation_cache = relu(Z)\n","```"]},{"cell_type":"markdown","metadata":{"id":"sexU0YLZSmWm"},"source":["- Lớp 1:\n","\n","$$\\textbf{A}^{(1)} = \\text{Relu}(\\textbf{Z}^{(1)}) $$\n","\n","- Lớp 2:\n","\n","$$\\textbf{A}^{(2)} = \\text{Relu}(\\textbf{Z}^{(2)}) $$\n","\n","- Lớp 3 (Lớp cuối):\n","\n","$$\\textbf{A}^{(3)} = \\hat{\\textbf{Y}} = \\text{Softmax}(\\textbf{Z}^{(3)}) \\tag{6}$$"]},{"cell_type":"code","metadata":{"id":"4WefOTnWuPC4"},"source":["def linear_activation_forward(A_prev, W, b, activation):\n","    \"\"\"\n","    Tiến hành non-linear forward\n","    A_prev, W và B đưa qua hàm linear_forward thu được Z\n","    Từ đó Z sẽ được đưa qua hàm phi tuyến RELU\n","    Đầu vào:\n","      A_prev:\n","        Dạng: numpy array\n","          Miêu tả: Đầu ra của lớp phía trước trên toàn bộ tập dữ liệu\n","          Chiều: (Chiều của lớp phía trước, Số lượng điểm dữ liệu)\n","      W:\n","        Dạng: numpy array\n","        Miêu tả: Ma trận tham số của lớp hiện tại\n","        Chiều: (Chiều của lớp hiện tại, Chiều của lớp phía trước)\n","      b:\n","        Dạng: numpy array\n","        Miêu tả: Vector bias của lớp hiện tại\n","        Chiều: (Chiều của lớp hiện tại, 1)\n","      activation:\n","        Dạng: python string\n","        Miêu tả: Tên hàm activation. Ví dụ \"softmax\" or \"relu\"\n","\n","    Returns:\n","      A:\n","        Dạng: numpy array\n","        Miêu tả: Đầu ra của phép phi tuyến hay đầu ra của hàm activation function\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","      cache:\n","        Dạng: python tuple: (linear_cache, activation_cache)\n","        Bao gồm:\n","          linear_cache: tuple (A_pre, W, b) đầu vào của phép tuyến tính được lưu lại\n","          activation_cache: Z đầu vào của phép phi tuyến được lưu lại\n","    \"\"\"\n","    # Ta sẽ phân loại việc biến đổi tuyến tính tùy theo đầu vào activation\n","    if activation == \"relu\":\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","    elif activation == \"softmax\":\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = softmax(Z)\n","\n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IabUnrabXmop"},"source":["Test code"]},{"cell_type":"code","metadata":{"id":"0jWyGRr_uPC7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c13e55bc-cf45-4fe2-b6cb-d0efbbf889d0"},"source":["try:\n","  A2 = np.ones((256, 60000))\n","  W3 = np.random.normal(size=((10, 256)))\n","  b3 = np.ones(((10, 1)))\n","\n","  A, cache = linear_activation_forward(A2, W3, b3, activation='softmax')\n","  print(\"A.shape: {}\".format(A.shape))\n","\n","  # cache là một tuple\n","\n","  linear_cache, activation_cache = cache\n","  A2, W3, b3 = linear_cache\n","\n","  print(\"activation_cache.shape hay Z.shape: {}\".format(activation_cache.shape))\n","  print(\"linear_cache bao gồm A2 với chiều A2.shape: {}, W3 với chiều W3.shape: {}, b3 với chiều b3.shape: {}\".format(A2.shape, W3.shape, b3.shape))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["A.shape: (10, 60000)\n","activation_cache.shape hay Z.shape: (10, 60000)\n","linear_cache bao gồm A2 với chiều A2.shape: (256, 60000), W3 với chiều W3.shape: (10, 256), b3 với chiều b3.shape: (10, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PwsazF-kuPDI"},"source":["## 5 - Hàm mất mát (Cost function)\n","\n","Công thức toán của cross-entropy cost $J$: $$J(\\textbf{W}) = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\sum\\limits_{j = 1}^{c} \\textbf{y}^{(i)}\\log (\\hat{\\textbf{y}}^{(i)})\\tag{7}$$\n","\n","Với\n","- $c$: số nhãn\n","- $m$: số điểm dữ liệu\n","\n","Chú ý trong bài này đầu vào của hàm là theo toàn bộ tập dữ liệu.\n"]},{"cell_type":"markdown","metadata":{"id":"KT_l4MKbNC9v"},"source":["**TODO 1**: Lập trình công thức hàm Cost Function"]},{"cell_type":"code","metadata":{"id":"_qYYnRcouPDI"},"source":["def compute_cost(Y_hat, Y):\n","    \"\"\"\n","    Tiến hành categorical cross-entropy trên toàn bộ tập dữ liệu\n","    Đầu vào:\n","    Y_hat:\n","      Dạng: numpy array\n","      Miêu tả: Đầu ra của lớp softmax trên toàn bộ tập dữ liệu\n","      Chiều: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","    Y:\n","      Dạng: numpy array\n","      Miêu tả: Nhãn của dữ liệu (Dạng One hot)\n","      Chiều: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","\n","    Returns:\n","      Dạng: numpy float\n","      Miêu tả: Giá trị mất mát trên toàn bộ tập dữ liệu\n","    \"\"\"\n","\n","    # Nhân Hadamard và lấy tổng theo cột.\n","    # Lấy tổng theo cột vì chiều đầu vào có số dòng là số lượng nhãn cho nên ta\n","    # cộng tất cả giá trị trong cùng 1 cột lại với nhau để tìm giá trị mất mát\n","    # trên 1 điểm dữ liệu đó\n","\n","    # Lập trình tại đây\n","\n","    m = None\n","\n","    cost = None\n","\n","    # Lấy tổng theo theo cột kèm theo chia trung bình\n","    cost = None\n","\n","    return cost"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7bhcwVHWYlgW"},"source":["Test code"]},{"cell_type":"code","metadata":{"id":"nbqLdQsjuPDK"},"source":["Y =  np.array(\n","      [[0., 0.],\n","       [0., 0.],\n","       [0., 1.],\n","       [0., 0.],\n","       [0., 0.],\n","       [1., 0.],\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.]])\n","\n","Y_hat =  np.array(\n","      [[0.1, 0.05],\n","       [0.1, 0.05],\n","       [0.1, 0.05],\n","       [0.2, 0.2],\n","       [0.05, 0.05],\n","       [0.2, 0.1],\n","       [0.1, 0.1],\n","       [0.05, 0.2],\n","       [0.05, 0.1],\n","       [0.05, 0.1]])\n","\n","print(\"cost = {}\".format(compute_cost(Y_hat, Y)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NmUY02AzNXqB"},"source":["**Kết quả mong đợi**:\n","\n","```\n","cost = 2.3025850929940455\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"iqtGNVroYm0h"},"source":["Giá trị này tương đương với $-(\\text{log}(0.2) + \\text{log}(0.05))$"]},{"cell_type":"markdown","metadata":{"id":"7f-Ms322uPDN"},"source":["## 6 - Lan truyền ngược (Backward propagation module)\n","\n","Cũng giống như lan truyền thuận (forward propagation), bây giờ bạn sẽ thực hiện các function cho backpropagation. Chú ý lan truyền thuận được sử dụng để tính toán gradient của hàm mất mát với các tham số (parameters).\n"]},{"cell_type":"markdown","metadata":{"id":"VAh6SysJZJ1Z"},"source":["Mỗi một hàm lan truyền thuận phía trên sẽ đi kèm một hàm lan truyền ngược"]},{"cell_type":"markdown","metadata":{"id":"axA8WuxqZizJ"},"source":["### 6.1 - Lan truyền ngược tuyến tính (Linear backward)"]},{"cell_type":"markdown","metadata":{"id":"lj8N7e2suPDN"},"source":["\n","\n","<img src=\"https://storage.googleapis.com/protonx-cloud-storage/images/Backprop3.PNG\" style=\"width:250px;height:300px;\">\n","<caption><center>Hình 4</center></caption>\n","\n","Tại lớp thứ $l$, ta có: $\\textbf{Z}^{(l)} = \\textbf{W}^{(l)} \\textbf{A}^{(l-1)} + \\textbf{b}^{(l)}$ (Activation tương ứng).\n","\n","Nhiệm vụ tiên quyết là tính được các giá trị $\\textbf{E}^{(l)}$ vì ta có thể dùng $\\textbf{E}^{(l)}$ để tính ra được $\\textbf{E}^{(l-1)}$\n","\n","Giá sử bạn đã tính đạo hàm của $\\textbf{E}^{(l)} = d\\textbf{Z}^{(l)} = \\frac{\\partial J }{\\partial \\textbf{Z}^{(l)}}$.\n","\n","\n","\n","Bạn muốn tính $(d\\textbf{W}^{(l)}, d\\textbf{b}^{(l)}, d\\textbf{A}^{(l-1)})$.\n","\n","3 giá trị $(d\\textbf{W}^{(l)}, d\\textbf{b}^{(l)}, d\\textbf{A}^{(l-1)}$ được tính toán từ $d\\textbf{Z}^{(l)}$ thông qua các công thức sau:\n","\n","\n","- Đạo hàm của hàm mất mát trên $\\textbf{W}^{(l)}$\n","\n","$$\n","d\\textbf{W}^{(l)} = \\frac{\\partial {J} }{\\partial \\textbf{W}^{(l)}} = \\frac{1}{m} d\\textbf{Z}^{(l)} \\textbf{A}^{(l-1) T} = \\frac{1}{m} \\textbf{E}^{(l)} \\textbf{A}^{(l-1) T} \\tag{8}\n","$$\n","\n","- Đạo hàm của hàm mất mát trên $\\textbf{b}^{(l)}$.Một cách cụ thể thì đây chính là trung bình tổng các cột của ma trận ${E}^{(l)}$\n","\n","\n","$$ d\\textbf{b}^{(l)} = \\frac{\\partial J }{\\partial \\textbf{b}^{(l)}} = \\frac{1}{m} \\sum_{i = 1}^{m} d\\textbf{Z}^{(l)(i)} = \\frac{1}{m} \\sum_{i = 1}^{m} \\textbf{E}^{(l)(i)} \\tag{9}$$\n","\n","      \n","- Đạo hàm của hàm mất mát trên $\\textbf{A}^{(l-1)}$\n","\n","$$ d\\textbf{A}^{(l-1)} = \\frac{\\partial J }{\\partial \\textbf{A}^{(l-1)}} = \\textbf{W}^{(l) T} d\\textbf{Z}^{(l)} = \\textbf{W}^{(l) T} \\textbf{E}^{(l)} \\tag{10}$$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QaXb8X5maclW"},"source":["**TODO 2:** Lập trình các công thức trên cho hàm lan truyền ngược tuyến tính"]},{"cell_type":"code","metadata":{"id":"9Qp5rYoquPDO"},"source":["def linear_backward(E, cache):\n","    \"\"\"\n","    Tính toán đạo hàm của hàm mất mát trên A, W và b\n","    Đầu vào:\n","      E hoặc dZ:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với đầu ra của phép tuyến tính tại lớp hiện tại (l)\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","      cache\n","        Dạng: Python tuple\n","        Miêu tả: Biến lưu trữ (cache) của lớp linear\n","        Bao gồm các biến A_prev, W và b\n","    Đầu ra:\n","      dA_prev:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với đầu ra phép phi tuyến của lớp l-1\n","        Chiều: (Chiều của lớp phía trước, Số lượng điểm dữ liệu)\n","      dW:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với tham số của lớp hiện tại (l)\n","        Chiều: (Chiều của lớp hiện tại, Chiều của lớp phía trước)\n","      db:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với bias của lớp hiện tại (l)\n","        Chiều: (Chiều của lớp hiện tại, 1)\n","    \"\"\"\n","    # Lập trình tại đây\n","\n","\n","    # Tính gradient trên W\n","    dW = None\n","\n","    # Tính gradient trên b\n","    db = None\n","\n","    # Tính gradient trên A phía trước\n","    dA_prev = None\n","\n","    return dA_prev, dW, db"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWjslJ75uPDS"},"source":["try:\n","  # Test trên layer 2\n","  E2 = dZ = np.ones((128, 60000))\n","  A1 = np.ones((256, 60000))\n","  W2 = np.ones((128, 256))\n","  b2 = np.ones((128,1))\n","  linear_cache = (\n","      A1, W2, b2\n","  )\n","\n","  dA1, dW2, db2 = linear_backward(E2, linear_cache)\n","  print('Chiều của dA1: {}'.format(dA1.shape))\n","  print('Chiều của dW2: {}'.format(dW2.shape))\n","  print('Chiều của db2: {}'.format(db2.shape))\n","  print('Giá trị dA1 được tính đúng?: {}'.format(np.array_equal(np.full(A1.shape, fill_value=128.), dA1)))\n","  print('Giá trị dW2 được tính đúng?: {}'.format(np.array_equal(np.full(W2.shape, fill_value=1.), dW2)))\n","  print('Giá trị db2 được tính đúng?: {}'.format(np.array_equal(np.full(b2.shape, fill_value=1.), db2)))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JBV-QJffuPDU"},"source":["**Kết quả mong đợi**:\n","\n","```\n","Chiều của dA1: (256, 60000)\n","Chiều của dW2: (128, 256)\n","Chiều của db2: (128, 1)\n","Giá trị dA1 được tính đúng?: True\n","Giá trị dW2 được tính đúng?: True\n","Giá trị db2 được tính đúng?: True\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"_Jpgrx_OfUhY"},"source":["### 6.2 - Lan truyền ngược phi tuyến (Activation backward)"]},{"cell_type":"markdown","metadata":{"id":"WuK2em4AfWe6"},"source":["**TODO 3**: Lập trình đạo hàm của RELU"]},{"cell_type":"markdown","metadata":{"id":"QEbW0Qh0uPDU"},"source":["\n","\n","\n","Nếu $f(.)$  là activation function `relu_backward` được tính: $$\\textbf{E}^{(l)} = d\\textbf{Z}^{(l)} = d\\textbf{A}^{(l)} * f'(\\textbf{Z}^{(l)})$$\n","\n","- Với **RELU**\n","$$f'(z) = \\left\\{\\begin{matrix}\n","0 \\ \\ \\forall z < 0 \\\\\n","1 \\ \\ \\forall z > 0 \\\\\n","DNE \\ \\ \\text{if} \\ \\ z = 0 \\\\\n","\\end{matrix}\\right.$$\n","\n","**RELU** `không có đạo` hàm tại 0 tuy nhiên khi lập trình ta sẽ cài đặt sẵn với trường hơp $z=0$ đạo hàm của RELU sẽ bằng `0`. Vậy công thức trở thành.\n","\n","$$f'(z) = \\left\\{\\begin{matrix}\n","0 \\ \\ \\forall z <= 0 \\\\\n","1 \\ \\ \\forall z > 0 \\\\\n","\\end{matrix}\\right.$$\n","\n","Cùng lập trình đạo hàm này với trình tự:\n","\n","- Tất cả những giá trị của $\\textbf{Z}$ nhỏ hơn hoặc bằng 0 sẽ được đặt thành 0\n","- Tất cả những giá trị của $\\textbf{Z}$ lớn hơn 0 sẽ được đặt thành 1"]},{"cell_type":"markdown","metadata":{"id":"0CR7gSLyDE7G"},"source":[]},{"cell_type":"code","metadata":{"id":"c4mvUgNJjOzk"},"source":["def relu_derivative(Z):\n","  \"\"\"\n","  Thực hiện đạo hàm của RELU trên ma trận Z\n","  Đầu vào:\n","    Z:\n","      Dạng: numpy array\n","      Miêu tả: Đầu ra của phép nhân tuyến tính và cũng là đầu vào của hàm activation\n","      Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","  Đầu ra:\n","    Z_grad:\n","      Dạng: numpy array\n","      Miêu tả: Đạo hàm của hàm RELU với Z\n","      Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","  \"\"\"\n","  # Lập trình tại đây\n","\n","  Z_grad = None\n","\n","  return Z_grad\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hdPfw7FPksJc"},"source":["Test code"]},{"cell_type":"code","metadata":{"id":"xNb9JCDSktI2"},"source":["try:\n","  # Test trên layer 2\n","  Z1 = np.full((128, 60000), 2.)\n","  Z2 = np.full((128, 60000), -2.)\n","\n","  Z1_derivative = relu_derivative(Z1)\n","  Z2_derivative = relu_derivative(Z2)\n","\n","\n","  print('Chiều của đạo hàm hàm RELU trên Z1: {}'.format(Z1_derivative.shape))\n","  print('Chiều của đạo hàm hàm RELU trên Z2: {}'.format(Z2_derivative.shape))\n","  print('Giá trị đạo hàm hàm RELU trên Z1 đã được tính đúng?: {}'.format(np.array_equal(Z1_derivative, np.ones(Z1.shape))))\n","  print('Giá trị đạo hàm hàm RELU trên Z2 đã được tính đúng?: {}'.format(np.array_equal(Z2_derivative, np.zeros(Z2.shape))))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S4uwd6jtmtK5"},"source":["**Kết quả mong đợi**:\n","\n","```\n","Chiều của đạo hàm hàm RELU trên Z1: (128, 60000)\n","Chiều của đạo hàm hàm RELU trên Z2: (128, 60000)\n","Giá trị đạo hàm hàm RELU trên Z1 đã được tính đúng?: True\n","Giá trị đạo hàm hàm RELU trên Z2 đã được tính đúng?: True\n","```"]},{"cell_type":"markdown","metadata":{"id":"_zNpCYprm3K2"},"source":["**TODO 4**: Lập trình hàm lan truyền ngược trên hàm phi tuyến\n","\n","Cụ thể là đạo hàm của **hàm mất mát** trên giá trị đầu vào hàm phi tuyến hay nói cách khác chính là:\n","\n","$$\\textbf{E}^{(l)} = d\\textbf{Z}^{(l)} = \\frac{\\partial J }{\\partial \\textbf{Z}^{(l)}} = d\\textbf{A}^{(l)} * f'(\\textbf{Z}^{(l)})$$"]},{"cell_type":"markdown","metadata":{"id":"a86EtnOZSx2k"},"source":["<img src=\"https://storage.googleapis.com/protonx-cloud-storage/backprop5.PNG\">"]},{"cell_type":"markdown","metadata":{"id":"KhPqOTFOnZPk"},"source":["Cho nên đầu vào của hàm tính giá trị $\\textbf{E}^{(l)}$ sẽ bao gồm 2 giá trị:\n","- Đạo hàm của hàm mất mát với đầu ra phép phi tuyến ở lớp hiện tại đã được tính trước: $d\\textbf{A}^{(l)}$\n","- Giá trị $\\textbf{Z}$ đã lưu lại khi thực hiện phi tuyến theo chiều thuận\n","\n","Hàm này sẽ thực hiện nhiệm vụ:\n","\n","- Sử dụng hàm `relu_derivative` để tính đạo hàm $f'(\\textbf{Z}^{(l)})$ của hàm RELU với $\\textbf{Z}$\n","- Sau đó lấy đầu vào $d\\textbf{A}^{(l)}$ nhân Hadamard với giá trị tìm được"]},{"cell_type":"code","metadata":{"id":"dgi9ZEUDpz9E"},"source":["def relu_backward(dA, cache):\n","    \"\"\"\n","    Thực hiện lan truyền ngược qua hàm phi tuyến RELU\n","    Hay nói cách khác đạo hàm của hàm mất mát trên Z\n","    Đầu vào:\n","      dA:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với đầu ra phép phi tuyến của lớp l\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","      cache:\n","        Dạng: numpy array\n","        Miêu tả: Z được lưu lại từ hàm linear-activation forward\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","    Đầu ra:\n","      E (dZ):\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của giá trị mất mát với Z\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","    \"\"\"\n","    # Lập trình tại đây\n","\n","    E = None\n","\n","    return E"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mLAd4St6TPYY"},"source":["Test Code"]},{"cell_type":"code","metadata":{"id":"vu4-QHRrTRCw"},"source":["try:\n","  dA1 = np.full((128, 60000), -2.)\n","  cache1 = np.full((128, 60000), -3.)\n","  E1 = relu_backward(dA1, cache1)\n","\n","  dA2 = np.full((128, 60000), 1.)\n","  cache2 = np.full((128, 60000), 1.)\n","  E2 = relu_backward(dA2, cache2)\n","\n","  print('Chiều của E1: {}'.format(E1.shape))\n","  print('Chiều của E2: {}'.format(E2.shape))\n","  print('Giá trị E1 đã được tính đúng?: {}'.format(np.array_equal(E1, np.full(E1.shape, 0.))))\n","  print('Giá trị E2 đã được tính đúng?: {}'.format(np.array_equal(E2, np.full(E2.shape, 1.))))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BxeqogguVgv-"},"source":["**Kết quả mong đợi**:\n","\n","```\n","Chiều của E1: (128, 60000)\n","Chiều của E2: (128, 60000)\n","Giá trị E1 đã được tính đúng?: True\n","Giá trị E2 đã được tính đúng?: True\n","```"]},{"cell_type":"markdown","metadata":{"id":"bcKXV9JwVrwD"},"source":["### 6.3 Lan truyền trên một lớp (Linear-Activation backward)"]},{"cell_type":"markdown","metadata":{"id":"qq68t8InVzYM"},"source":["**TODO 5:** Kết hợp phần `6.1` và `6.2` để tạo thành một hàm đầy đủ việc lan truyền ngược trên một lớp ẩn bất kỳ. Chú ý **đường màu xanh**."]},{"cell_type":"markdown","metadata":{"id":"detEtiDAc5rQ"},"source":["<img src=\"https://storage.googleapis.com/protonx-cloud-storage/Backprop7.PNG\" >"]},{"cell_type":"code","metadata":{"id":"Rc4-K95KuPDV"},"source":["def linear_activation_backward(dA, cache, activation):\n","    \"\"\"\n","    Thực hiện lan truyền ngược trên lớp này\n","    Sử dụng kết hợp relu_backward và hàm linear_backward\n","    Đầu vào:\n","      dA:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với đầu ra phép phi tuyến của lớp l\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","      cache:\n","        Dạng: Python tuple\n","        Miêu tả: (linear_cache, activation_cache)\n","        Bao gồm:\n","          linear_cache:\n","            Dạng: Python tuple\n","            Miêu tả: cache lưu lại từ phép tuyến tính thuận: (A_pre, W, b)\n","          activation_cache:\n","            Dạng: Numpy array\n","            Miêu tả: cache lưu lại từ phép phi tính thuận: (Z)\n","      activation:\n","        Dạng: Python string\n","        Miêu tả: Tên của activation\n","        Ví dụ: \"relu\"\n","    Đầu ra:\n","      dA_prev:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với đầu ra phép phi tuyến của lớp l-1\n","        Chiều: (Chiều của lớp phía trước, Số lượng điểm dữ liệu)\n","      dW:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với tham số của lớp hiện tại (l)\n","        Chiều: (Chiều của lớp hiện tại, Chiều của lớp phía trước)\n","      db:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với bias của lớp hiện tại (l)\n","        Chiều: (Chiều của lớp hiện tại, 1)\n","    \"\"\"\n","    # Lập trình tại đây\n","\n","    linear_cache, activation_cache = cache\n","\n","    # Thực hiện activation backward\n","    # Từ dA và activation_cache để tính ra dZ\n","    if activation == \"relu\":\n","        dZ = None\n","\n","    # Hàm này có thể thiết kế để sử dụng nhiều hàm khác nhau trong tương lai\n","    # Bạn không cần code elif này\n","    elif activation == \"gelu\":\n","        pass\n","\n","    # Thực hiện linear backward\n","    # Từ dZ và linear_cache để tính ra dA_prev, dW và db\n","    dA_prev, dW, db = None\n","\n","    return dA_prev, dW, db"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oaOh_-h9Wxat"},"source":["Test code"]},{"cell_type":"code","metadata":{"id":"5aOjEaskWzKN"},"source":["try:\n","  np.random.seed(42)\n","  dA2 = np.full((128, 60000), 4.)\n","  activation = 'relu'\n","  A1 = np.ones((256, 60000))\n","  W2 = np.full((128, 256), 2.)\n","  b2 = np.ones((128,1))\n","  linear_cache = (\n","      A1, W2, b2\n","  )\n","  activation_cache = Z = np.full((128, 60000), 2.5)\n","  cache = (linear_cache, activation_cache)\n","\n","  dA1, dW, db = linear_activation_backward(dA2, cache, activation)\n","\n","\n","  print('Chiều của dA1: {}'.format(dA1.shape))\n","  print('Chiều của dW: {}'.format(dW.shape))\n","  print('Chiều của db: {}'.format(db.shape))\n","  print('Giá trị dA1 đã được tính đúng?: {}'.format(np.array_equal(dA1, np.full(dA1.shape, 1024.))))\n","  print('Giá trị dW đã được tính đúng?: {}'.format(np.array_equal(dW, np.full(W2.shape, 4.))))\n","  print('Giá trị db đã được tính đúng?: {}'.format(np.array_equal(E2, np.full(E2.shape, 1.))))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CzcH7WU1Qrk6"},"source":["**Kết quả mong đợi**:\n","\n","```\n","Chiều của dA1: (256, 60000)\n","Chiều của dW: (128, 256)\n","Chiều của db: (128, 1)\n","Giá trị dA1 đã được tính đúng?: True\n","Giá trị dW đã được tính đúng?: True\n","Giá trị db đã được tính đúng?: True\n","```"]},{"cell_type":"markdown","metadata":{"id":"lHXPbi_juPDe"},"source":["### 6.4 - Cập nhật tham số\n","\n","Trong phần này, bạn sẽ thực hiện cập nhật các tham số:\n","\n","\n","$$ \\textbf{W}^{(l)} = \\textbf{W}^{(l)} - \\alpha \\text{ } d\\textbf{W}^{(l)} \\tag{16}$$\n","$$ \\textbf{b}^{(l)} = \\textbf{b}^{(l)} - \\alpha \\text{ } d\\textbf{b}^{(l)} \\tag{17}$$\n","\n","trong đó $\\alpha$ là learning rate. Sau khi tính cập nhật giá trị Parameters, chúng ta sẽ lưu trữ trong trong 1 parameters dictionary."]},{"cell_type":"markdown","metadata":{"id":"RmDVbzIbuPDe"},"source":["**TODO 6**: Thực hiện `update_parameters()`.\n","\n","**Hướng dẫn**: Cập nhật tham số sử dụng Gradient Descent cho $\\textbf{W}^{(l)}$ và $\\textbf{b}^{(l)}$ của từng layer $l = 1, 2, ..., L$.\n"]},{"cell_type":"code","metadata":{"id":"Aug2qIkVuPDe"},"source":["def update_parameters(parameters, grads, learning_rate):\n","    \"\"\"\n","    Lặp qua các lớp và cập nhật tham số sử dụng Gradient Descent\n","    Đầu vào:\n","      parameters:\n","        Dạng: Python dictionary\n","        Miêu tả: Chứa các tham số với key là tên và giá trị là numpy array tham số đó\n","        Ví dụ: {\n","          W1: ...,\n","          b1: ...,\n","          ...\n","        }\n","      grads:\n","        Dạng: Python dictionary\n","        Miêu tả: Chứa gradient của các tham số với key là tên gradient và giá trị là numpy array của gradient\n","        Ví dụ: {\n","          dW1: ...,\n","          db1: ...,\n","          ...\n","        }\n","    Đầu ra:\n","      parameters:\n","        Dạng: Python dictionary\n","        Miêu tả: Chứa các tham số đã được cập nhật gradient\n","        Ví dụ: {\n","          W1: ...,\n","          b1: ...,\n","          ...\n","        }\n","    \"\"\"\n","    # Lập trình tại đây\n","\n","    parameters = None\n","\n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVt3Fqnl7UhJ"},"source":["Test code"]},{"cell_type":"code","metadata":{"id":"WBlqTwWD7XH5"},"source":["try:\n","  n = 784\n","  d_1 = 128\n","  d_2 = 256\n","  d_3 = 1028\n","  np.random.seed(42)\n","  parameters = {\n","      \"W1\": np.random.randn(d_1, n),\n","      \"b1\": np.zeros((d_1, 1)),\n","      \"W2\": np.random.randn(d_2, d_1),\n","      \"b2\": np.zeros((d_2, 1)),\n","      \"W3\": np.random.randn(d_3, d_2),\n","      \"b3\": np.zeros((d_3, 1)),\n","  }\n","\n","  grads = {\n","      \"dW1\": np.full((d_1, n), -0.02),\n","      \"db1\": np.full((d_1, 1), 0.1),\n","      \"dW2\": np.full((d_2, d_1), -0.4),\n","      \"db2\": np.full((d_2, 1), 0.5),\n","      \"dW3\": np.full((d_3, d_2), 0.6),\n","      \"db3\": np.full((d_3, 1), -0.02),\n","  }\n","\n","  lr = 0.01\n","  updated_parameters = update_parameters(parameters, grads, lr)\n","  exp = {'W1': 106.015, 'b1': -0.128, 'W2': 200.643, 'b2': -1.28, 'W3': -1661.189, 'b3': 0.206}\n","  for key in updated_parameters:\n","    print(\"{} được cập nhật đúng?: {}\".format(key, exp[key] == np.round(np.sum(updated_parameters[key]), 3)))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Y-BfyHGQt2S"},"source":["**Kết quả mong đợi**:\n","\n","```\n","W1 được cập nhật đúng?: True\n","b1 được cập nhật đúng?: True\n","W2 được cập nhật đúng?: True\n","b2 được cập nhật đúng?: True\n","W3 được cập nhật đúng?: True\n","b3 được cập nhật đúng?: True\n","```"]},{"cell_type":"markdown","metadata":{"id":"anM9Ho3fMPVo"},"source":["##  7 - Train Model"]},{"cell_type":"markdown","metadata":{"id":"osy4dRRiRa1E"},"source":["### 7.1. Tải dữ liệu MNIST"]},{"cell_type":"code","metadata":{"id":"iVlUEU7pceyr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6b030c5c-9a0c-4ab3-b98d-b931791ad8fb"},"source":["import tensorflow as tf\n","(X_train, Y_train), (X_val, Y_val) = tf.keras.datasets.mnist.load_data()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e2mD77mIutDs"},"source":["Một số thông tin quan trọng"]},{"cell_type":"code","metadata":{"id":"vzRTUc-cuu_6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3e799ba1-0663-43e7-ce34-b148b5a8d6c0"},"source":["# Số lượng nhãn\n","num_classes = 10\n","\n","num_of_train_images, width, height = X_train.shape\n","\n","# Chiều ảnh được duỗi\n","image_vector_size = width * height\n","\n","\n","print(\"\"\"\n","  Số lượng ảnh train: {}\n","  Chiều dài ảnh train: {}\n","  Chiều cao ảnh train: {}\n","  Chiều ảnh được duỗi: {}\n","\"\"\".format(num_of_train_images, width, height, image_vector_size))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","  Số lượng ảnh train: 60000\n","  Chiều dài ảnh train: 28\n","  Chiều cao ảnh train: 28\n","  Chiều ảnh được duỗi: 784\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"egzxTzYNRfEV"},"source":["### 7.2. Tiền xử lý dữ liệu"]},{"cell_type":"markdown","metadata":{"id":"Is0v-oBj0b8t"},"source":["Các hàm này được sử dụng từ bài Softmax Regression, nếu bạn chưa hiểu kỹ bài tập này thì hãy làm bài tập đó trước nhé ;)"]},{"cell_type":"code","metadata":{"id":"FE1ZkXaoWgIl"},"source":["def one_hot(y, num_classes):\n","  \"\"\"\n","  Đầu vào:\n","    y:\n","      Dạng: numpy array\n","      Miêu tả: Các giá trị nhãn\n","      Chiều: (batch_size,)\n","    num_classes:\n","      Dạng: Python integer\n","      Miêu tả: Số lượng nhãn\n","      Điều kiện: num_classes > 0\n","      Ví dụ: 10\n","  Đầu ra:\n","    y_one_hot:\n","      Miêu tả: Trả về ma trận các vector one hot của từng nhãn\n","      Chiều: (batch_size, num_classes)\n","  \"\"\"\n","  y_one_hot = np.squeeze(np.eye(num_classes)[y.reshape(-1)])\n","\n","  return y_one_hot\n","\n","def flatten_images(images):\n","  \"\"\"\n","  Thực thi duỗi ảnh từ (batch_size, width, height) thành (batch_size, width x height)\n","  Đầu vào:\n","    images:\n","      Dạng: numpy array\n","      Miêu tả: Ma trận các ảnh\n","      Chiều: (batch_size, width, height)\n","      Ví dụ: (32, 28, 28)\n","  Đầu ra:\n","    flattened_images\n","      Miêu tả: ma trận trong đó các ảnh được duỗi thành vector\n","      Chiều: (batch_size, image_vector_size) = (batch_size, width x height)\n","      Ví dụ: (32, 28 x 28) = (32, 784)\n","  \"\"\"\n","  flattened_images = np.reshape(images, (images.shape[0], -1))\n","\n","  return flattened_images\n","\n","def normalize_images(images):\n","  \"\"\"\n","  Hàm chuẩn hóa ảnh các giá trị pixel để nằm trong khoảng từ 0 đến 1\n","  Đầu vào:\n","    images:\n","      Dạng: numpy array\n","      Miêu tả: Ma trận các vector ảnh\n","      Chiều: (batch_size, image_vector_size)\n","      Ví dụ: (32, 784)\n","  Đầu ra:\n","    normailized_images\n","      Miêu tả: ma trận trong đó các vector ảnh được chuẩn hóa\n","      Chiều: (batch_size, image_vector_size)\n","      Ví dụ: (32, 784)\n","  \"\"\"\n","  normailized_images = images / 255.0\n","\n","  return normailized_images\n","\n","try:\n","  X_train = flatten_images(X_train)\n","  X_val = flatten_images(X_val)\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)\n","\n","try:\n","  X_train = normalize_images(X_train)\n","  X_val = normalize_images(X_val)\n","except Exception as e:\n","   print(\"Lỗi thực thi: \", e)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AqztuH9zeTOe"},"source":["### 7.3. Định nghĩa chiều các lớp"]},{"cell_type":"code","metadata":{"id":"4btCQmPABl0B"},"source":["n = image_vector_size\n","d_1 = 1024\n","d_2 = 256\n","c = 10\n","layers_dims = (n, d_1, d_2, c)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"29Y2QRPqtPVH"},"source":["### 7.4. Xây dựng mô hình"]},{"cell_type":"markdown","metadata":{"id":"jxhPHl7metoN"},"source":["**TODO 7**: Lập trình hàm tính độ chính xác"]},{"cell_type":"markdown","metadata":{"id":"2yNLprEw0qoT"},"source":["Trong trường hợp này trong thân hàm cần chú ý tới chiều\n","\n","\n","- $n: $ chiều vector ảnh (784)\n","- $m: $ số ảnh (60000)\n","- $c: $ số nhãn (10)\n","\n","Chiều của $\\textbf{X} \\in \\mathbf{R} ^ {n \\times m}$\n","Chiều của $\\textbf{Y} \\in \\mathbf{R} ^ {m \\times 1}$\n"]},{"cell_type":"code","metadata":{"id":"xJHHg064vgUa"},"source":["def cal_acc(parameters, X, Y):\n","  \"\"\"\n","  Tính độ chính xác của mô hình trên toàn bộ tập dữ liệu\n","    Bước 1: Thực hiện feed forward để lấy kết quả dự đoán\n","    Bước 2: So sánh kết quả dự đoán với nhãn thật\n","  Đầu vào:\n","    X:\n","      Dạng: numpy array\n","      Miêu tả: Các vector ảnh\n","      Chiều: (image_vector_size, Số lượng điểm dữ liệu)\n","      Ví dụ: (784, 60000)\n","    Y:\n","      Dạng: numpy arry\n","      Miêu tả: Vector của nhãn\n","      Chiều: (Số lượng điểm dữ liệu,)\n","      Ví dụ: (60000,)\n","  Đầu ra:\n","    acc:\n","      Dạng: số thực\n","      Miêu tả: Độ chính xác của mô hình tại thời điểm hiện tại\n","      Ví dụ: 0.6\n","  \"\"\"\n","  # Lập trình tại đây\n","\n","  acc = None\n","\n","  return acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rg0DUEp_OpZv"},"source":["Test code"]},{"cell_type":"code","metadata":{"id":"Fhz9UoQzOsWo"},"source":["try:\n","  np.random.seed(42)\n","  parameters = {\n","    \"W1\": np.random.randn(d_1, n),\n","    \"b1\": np.zeros((d_1, 1)),\n","    \"W2\": np.random.randn(d_2, d_1),\n","    \"b2\": np.zeros((d_2, 1)),\n","    \"W3\": np.random.randn(num_classes, d_2),\n","    \"b3\": np.zeros((num_classes, 1)),\n","  }\n","\n","  Y_mock =  np.array([5,2,6, 4])\n","  X_mock = X_mock = X_train[:4].T / 255.0\n","  acc_mock = cal_acc(parameters, X_mock, Y_mock)\n","  print(\"Độ chính xác: {:.3f}\".format(acc_mock))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MzJuCLa9Q2-2"},"source":["**Kết quả mong đợi**:\n","\n","```\n","Độ chính xác: 0.500\n","```"]},{"cell_type":"markdown","metadata":{"id":"YT0oevf9qxtN"},"source":["<img src=\"https://storage.googleapis.com/protonx-cloud-storage/images/Backprop4.PNG\"  />"]},{"cell_type":"markdown","metadata":{"id":"rEs4jQG9e0Mm"},"source":["Với 2 lớp ta sẽ có các công thức sau.\n","\n","\n","**Chiều Backward:**\n","\n","- Lớp Softmax cuối cùng:\n","\n","$$\\textbf{E}^{(3)} = \\hat{\\textbf{Y}} - \\textbf{Y} $$\n","\n","$$ d\\textbf{A}^{(2)} = \\textbf{W}^{(3)T}\\textbf{E}^{(3)} $$\n","\n","\n","$$ d\\textbf{W}^{(3)} = \\frac{1}{m} \\textbf{E}^{(3)}\\textbf{A}^{(2)T} \\rightarrow \\textbf{W}^{(3)} := \\textbf{W}^{(3)}  - \\alpha \\frac{1}{m} \\textbf{E}^{(3)}\\textbf{A}^{(2)T}    $$\n","\n","$$ d\\textbf{b}^{(3)} = \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(3)(i)}\\rightarrow \\textbf{b}^{(3)} := \\textbf{b}^{(3)}  - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(3)(i)}$$\n","\n","- Lớp ẩn số 2:\n","\n","$$\\textbf{E}^{(2)} = d\\textbf{Z}^{(2)} = {d\\textbf{A}^{(2)}} \\odot f'(\\textbf{Z}^{(2)}) $$\n","\n","$$ d\\textbf{A}^{(1)} = \\textbf{W}^{(2)T}\\textbf{E}^{(2)} $$\n","\n","$$ d\\textbf{W}^{(2)} = \\frac{1}{m} \\textbf{E}^{(2)}\\textbf{A}^{(1)T} \\rightarrow \\textbf{W}^{(2)} := \\textbf{W}^{(2)}  - \\alpha \\frac{1}{m} \\textbf{E}^{(2)}\\textbf{A}^{(1)T}    $$\n","\n","$$ d\\textbf{b}^{(2)} = \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(2)(i)}\\rightarrow \\textbf{b}^{(2)} := \\textbf{b}^{(2)}  - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(2)(i)}$$\n","\n","\n","- Lớp ẩn số 1:\n","\n","$$\\textbf{E}^{(1)} = d\\textbf{Z}^{(1)} = {d\\textbf{A}^{(1)}} \\odot f'(\\textbf{Z}^{(1)}) $$\n","\n","$$ d\\textbf{W}^{(1)} = \\frac{1}{m} \\textbf{E}^{(1)}\\textbf{X}^{T} \\rightarrow \\textbf{W}^{(1)} := \\textbf{W}^{(1)}  - \\alpha \\frac{1}{m} \\textbf{E}^{(1)}\\textbf{X}^{T}    $$\n","\n","$$ d\\textbf{b}^{(1)} = \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(1)(i)}\\rightarrow \\textbf{b}^{(1)} := \\textbf{b}^{(1)}  - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(1)(i)}$$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lt1hTST0Kc-F"},"source":["Nếu bạn thắc mắc tại sao $$\\textbf{E}^{(3)} = \\hat{\\textbf{Y}} - \\textbf{Y} $$\n","\n","thì lời giải nằm [tại đây](https://colab.research.google.com/drive/1awivtxQzjgNK2m-cEQLIyzE5g6MxcZpX#scrollTo=NYrcc6YWGz1S)."]},{"cell_type":"markdown","metadata":{"id":"yi2yxBGsTdHR"},"source":["**TODO 8:** Xây dựng hàm train model"]},{"cell_type":"code","metadata":{"id":"OnePW1bgNNv0"},"source":["def train_model(X_train, Y_train, X_val, Y_val, layers_dims, learning_rate=0.1, epochs=250):\n","    \"\"\"\n","    Hàm xây dựng và train model\n","      Bước 1: Thực hiện lan truyền thuận, tính giá trị đầu ra của các lớp\n","      Bước 2: Tính giá trị mất mát\n","      Bước 3: Thực hiện lan truyền ngược, tính giá trị đạo hàm của hàm mất mát trên các tham số\n","      Bước 4: Cập nhật tham số của mô hình\n","    Đầu vào:\n","      X_train:\n","        Dạng: numpy array\n","        Miêu tả: Các vector ảnh\n","        Chiều: (Số lượng điểm dữ liệu, image_vector_size)\n","        Ví dụ: (60000, 784)\n","      Y_train:\n","        Dạng: numpy arry\n","        Miêu tả: Vector của nhãn\n","        Chiều: (Số lượng điểm dữ liệu,)\n","        Ví dụ: (60000,)\n","      X_val:\n","        Dạng: numpy array\n","        Miêu tả: Các vector ảnh\n","        Chiều: (Số lượng điểm dữ liệu, image_vector_size)\n","        Ví dụ: (10000, 784)\n","      Y_val:\n","        Dạng: numpy arry\n","        Miêu tả: Vector của nhãn\n","        Chiều: (Số lượng điểm dữ liệu,)\n","        Ví dụ: (10000,)\n","      layers_dims:\n","        Dạng: Python tuple\n","        Miêu tả: (n, d_1, d_2, num_classes)\n","          n: Chiều của vector ảnh\n","          d_1: Chiều của lớp ẩn 1\n","          d_2: Chiều của lớp ẩn 2\n","          num_classes: số nhãn\n","      learning_rate:\n","        Dạng: Python float\n","        Miêu tả: Tốc độ học của mô hình\n","        Ví dụ: 0.001\n","      epochs:\n","        Dạng: Python integer\n","        Miêu tả: Số vòng lặp qua tập dữ liệu\n","        Ví dụ: 100\n","    Đầu ra:\n","      Python Tuple: (parameters, acc, val_acc, costs)\n","      parameters:\n","        Dạng: Python Dictionary\n","        Miêu tả: Bộ tham số của mô hình\n","      acc:\n","        Dạng: số thực\n","        Miêu tả: Độ chính xác của mô hình trên bộ train\n","        Ví dụ: 0.6\n","      val_acc:\n","        Dạng: số thực\n","        Miêu tả: Độ chính xác của mô hình trên bộ validation\n","        Ví dụ: 0.7\n","      costs:\n","        Dạng: Python List\n","        Miêu tả: Giá trị mất mát trên từng epoch\n","        Ví dụ [0.2, 0.15, 0.1]\n","    \"\"\"\n","\n","    np.random.seed(42)\n","    grads = {}\n","    costs = []\n","    m = X_train.shape[0]\n","    (n, d_1, d_2, num_classes) = layers_dims\n","\n","    # 0. Khởi tạo tham số\n","    parameters = None\n","\n","    # 1. Điều chỉnh chiều dữ liệu sao cho\n","    # X_train: (image_vector_size, Số lượng điểm dữ liệu)\n","    # Y_train_one_hot: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","    # X_val: (image_vector_size, Số lượng điểm dữ liệu)\n","\n","    X_train = None\n","    Y_train_one_hot = None\n","    X_val = None\n","\n","\n","    # 2. Lấy các tham số ra khỏi dictionary: parameters\n","\n","    # 3. Lấy các tham số ra khỏi dictionary: parameters\n","    for i in range(0, epochs):\n","\n","        # 3.1. Thực hiện Feed Forward trên 3 lớp: 2 lớp relu và lớp Softmax\n","\n","        # 3.2. Tính giá trị mất mát\n","\n","        # 3.3. Tính giá trị E3\n","\n","        # 3.4. Tính giá trị dA2\n","\n","        # 3.5. Tính giá trị dW3\n","\n","        # 3.6. Tính giá trị db3\n","\n","        # Tính Accuracy\n","\n","        # 3.7. Tính độ chính xác trên tập train\n","\n","        # 3.8. Tính độ chính xác trên tập validation\n","\n","        # 3.9. Thực hiện backward trên lớp số 2 để tính ra dA1, dW2, db2\n","\n","        # 3.10. Thực hiện backward trên lớp số 1 để tính ra dA0, dW1, db1\n","\n","        # 3.11. Cập nhật các giá trị dW1, db1, dW2, db2, dW3, db3 vào dictionary grads\n","\n","        # 3.12. Tiến hành cập nhật các tham số của mô hình\n","\n","        # 3.13. Gán các giá trị tham số mới vào các biến đã được định nghĩa sẵn\n","\n","\n","        print(\"Epoch {}: Cost: {:.3f} Acc: {:.3f} Validation Acc: {:.3f}\".format(i + 1, np.squeeze(cost), acc, val_acc))\n","\n","        costs.append(cost)\n","\n","\n","    return parameters, acc, val_acc, costs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ACVkTBjjTkLH"},"source":["### 7.5. Tiến hành training"]},{"cell_type":"markdown","metadata":{"id":"VRp0LjUdQbG6"},"source":["**TODO 9**: Tiến hành training"]},{"cell_type":"code","metadata":{"id":"HE_sYT2bSYFW"},"source":["try:\n","  # plot the cost\n","  epochs = 400\n","  learning_rate = 0.1\n","  parameters, acc, val_acc, costs  = train_model(X_train, Y_train, X_val, Y_val, layers_dims = layers_dims, learning_rate=learning_rate, epochs=epochs)\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0KNolg09TuSI"},"source":["**Kết quả mong đợi (Tương đối)**:\n","\n","```\n","Epoch 387: Cost: 0.421 Acc: 0.880 Validation Acc: 0.882\n","Epoch 388: Cost: 0.420 Acc: 0.881 Validation Acc: 0.883\n","Epoch 389: Cost: 0.420 Acc: 0.881 Validation Acc: 0.883\n","Epoch 390: Cost: 0.419 Acc: 0.881 Validation Acc: 0.883\n","Epoch 391: Cost: 0.418 Acc: 0.881 Validation Acc: 0.884\n","Epoch 392: Cost: 0.418 Acc: 0.881 Validation Acc: 0.884\n","Epoch 393: Cost: 0.417 Acc: 0.882 Validation Acc: 0.884\n","Epoch 394: Cost: 0.416 Acc: 0.882 Validation Acc: 0.884\n","Epoch 395: Cost: 0.416 Acc: 0.882 Validation Acc: 0.884\n","Epoch 396: Cost: 0.415 Acc: 0.882 Validation Acc: 0.884\n","Epoch 397: Cost: 0.414 Acc: 0.882 Validation Acc: 0.884\n","Epoch 398: Cost: 0.414 Acc: 0.882 Validation Acc: 0.884\n","Epoch 399: Cost: 0.413 Acc: 0.883 Validation Acc: 0.884\n","Epoch 400: Cost: 0.412 Acc: 0.883 Validation Acc: 0.885\n","```"]},{"cell_type":"markdown","metadata":{"id":"4WyVklEEKNFp"},"source":["Chúc mừng bạn đã vượt qua một thử thách lớn trong con đường học AI của mình."]},{"cell_type":"markdown","metadata":{"id":"8Fux1mJWKeqU"},"source":["<img src=\"https://storage.googleapis.com/protonx-cloud-storage/icons/488-bicycle-outline.gif\" />"]}]}